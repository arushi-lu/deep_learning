{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1cUINHAGI6TTGjFweOsFMXBgpij7c8Ap6",
      "authorship_tag": "ABX9TyN4fdkOfRn8QwZViAl5n8UN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arushi-lu/deep_learning/blob/main/CNN_PPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing -> creating 5 folds\n",
        "\n",
        "From the original dataset (127260) ->\n",
        "\n",
        "Used 50000 samples for training and validation\n",
        "\n",
        "and  10000 samples for testing"
      ],
      "metadata": {
        "id": "iP573MsPoELP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "AeuyJaReoJhr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = '1IxN2sX2TX0uK6CFDh8eudb8haz3RlF7X'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output_file = 'data.hdf5'\n",
        "\n",
        "gdown.download(download_url, output_file, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "MFblS6PaoLq6",
        "outputId": "ee626c2a-b88e-43b9-efe0-75f75b724834"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1IxN2sX2TX0uK6CFDh8eudb8haz3RlF7X\n",
            "From (redirected): https://drive.google.com/uc?id=1IxN2sX2TX0uK6CFDh8eudb8haz3RlF7X&confirm=t&uuid=43f4e7b4-a17f-4309-a91f-53b41753975c\n",
            "To: /content/data.hdf5\n",
            "100%|██████████| 2.55G/2.55G [00:25<00:00, 101MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data.hdf5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "# Load the HDF5 file\n",
        "file_path = 'data.hdf5'\n",
        "with h5py.File(file_path, 'r') as f:\n",
        "    # List all groups and datasets within the file\n",
        "    print(\"Keys in 'data.hdf5':\")\n",
        "    print(list(f.keys()))\n",
        "\n",
        "    # Assuming there's a dataset named 'data' containing all samples\n",
        "    dataset = f['data']\n",
        "\n",
        "    # Check the shape of the dataset to understand its size\n",
        "    print(\"Shape of 'data' dataset:\", dataset.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxqzAwFqoPCe",
        "outputId": "4b2b3242-5899-4338-eb66-121f5b37e00f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in 'data.hdf5':\n",
            "['data']\n",
            "Shape of 'data' dataset: (127260, 2, 1250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "7p6PcGdxr6QH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data handling: Main function to create 5 folds for cross-validation"
      ],
      "metadata": {
        "id": "GXgA8JpCKBKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "def fold_data():\n",
        "    length = 1250  # length of the signals\n",
        "\n",
        "    # Starting points of validation data for 5 folds\n",
        "    validation_data_start = {\n",
        "        0: 40000,\n",
        "        1: 0,\n",
        "        2: 10000,\n",
        "        3: 20000,\n",
        "        4: 30000,\n",
        "    }\n",
        "\n",
        "    # Load the episode data once\n",
        "    fl = h5py.File('data.hdf5', 'r')\n",
        "    data = fl['data'][:50000]\n",
        "\n",
        "    for fold_id in tqdm(range(5), desc='Folding Data'):  # Iterate for 5 folds\n",
        "        X_train = []  # Initialize train data\n",
        "        Y_train = []\n",
        "\n",
        "        X_val = []  # Initialize validation data\n",
        "        Y_val = []\n",
        "\n",
        "        max_ppg = -10000  # Initialize metadata for min-max of abp, ppg signals\n",
        "        min_ppg = 10000\n",
        "        max_abp = -10000\n",
        "        min_abp = 10000\n",
        "\n",
        "        val_start = validation_data_start[fold_id]  # Validation data start\n",
        "        val_end = val_start + 10000  # Validation data end\n",
        "\n",
        "        # Process training data before validation samples\n",
        "        for i in tqdm(range(0, val_start), desc='Training Data Part 1'):\n",
        "            sample = data[i]\n",
        "            X_train.append(sample[1][:length].reshape(length, 1))  # ppg signal\n",
        "            Y_train.append(sample[0][:length].reshape(length, 1))  # abp signal\n",
        "            max_ppg = max(np.max(sample[1]), max_ppg)\n",
        "            min_ppg = min(np.min(sample[1]), min_ppg)\n",
        "            max_abp = max(np.max(sample[0]), max_abp)\n",
        "            min_abp = min(np.min(sample[0]), min_abp)\n",
        "\n",
        "        # Process training data after validation samples\n",
        "        for i in tqdm(range(val_end, 50000), desc='Training Data Part 2'):\n",
        "            sample = data[i]\n",
        "            X_train.append(sample[1][:length].reshape(length, 1))  # ppg signal\n",
        "            Y_train.append(sample[0][:length].reshape(length, 1))  # abp signal\n",
        "            max_ppg = max(np.max(sample[1]), max_ppg)\n",
        "            min_ppg = min(np.min(sample[1]), min_ppg)\n",
        "            max_abp = max(np.max(sample[0]), max_abp)\n",
        "            min_abp = min(np.min(sample[0]), min_abp)\n",
        "\n",
        "        # Process validation data\n",
        "        for i in tqdm(range(val_start, val_end), desc='Validation Data'):\n",
        "            sample = data[i]\n",
        "            X_val.append(sample[1][:length].reshape(length, 1))  # ppg signal\n",
        "            Y_val.append(sample[0][:length].reshape(length, 1))  # abp signal\n",
        "            max_ppg = max(np.max(sample[1]), max_ppg)\n",
        "            min_ppg = min(np.min(sample[1]), min_ppg)\n",
        "            max_abp = max(np.max(sample[0]), max_abp)\n",
        "            min_abp = min(np.min(sample[0]), min_abp)\n",
        "\n",
        "        # Convert lists to numpy arrays for efficiency\n",
        "        X_train = np.array(X_train)\n",
        "        Y_train = np.array(Y_train)\n",
        "        X_val = np.array(X_val)\n",
        "        Y_val = np.array(Y_val)\n",
        "\n",
        "        # Normalize training and validation data\n",
        "        X_train = (X_train - min_ppg) / (max_ppg - min_ppg)\n",
        "        Y_train = (Y_train - min_abp) / (max_abp - min_abp)\n",
        "        X_val = (X_val - min_ppg) / (max_ppg - min_ppg)\n",
        "        Y_val = (Y_val - min_abp) / (max_abp - min_abp)\n",
        "\n",
        "        # Save training and validation data splits\n",
        "        os.makedirs('data', exist_ok=True)\n",
        "        with open(f'data/train{fold_id}.p', 'wb') as f:\n",
        "            pickle.dump({'X_train': X_train, 'Y_train': Y_train}, f)\n",
        "        with open(f'data/val{fold_id}.p', 'wb') as f:\n",
        "            pickle.dump({'X_val': X_val, 'Y_val': Y_val}, f)\n",
        "\n",
        "        # Save metadata\n",
        "        with open(f'data/meta{fold_id}.p', 'wb') as f:\n",
        "            pickle.dump({'max_ppg': max_ppg, 'min_ppg': min_ppg,\n",
        "                         'max_abp': max_abp, 'min_abp': min_abp}, f)\n",
        "\n",
        "    # Process test data\n",
        "    fl = h5py.File('data.hdf5', 'r')\n",
        "    test_data = fl['data'][50000:60000]\n",
        "    X_test = []\n",
        "    Y_test = []\n",
        "\n",
        "    for sample in tqdm(test_data, desc='Test Data'):\n",
        "        X_test.append(sample[1][:length].reshape(length, 1))  # ppg signal\n",
        "        Y_test.append(sample[0][:length].reshape(length, 1))  # abp signal\n",
        "\n",
        "    fl.close()  # Close the HDF5 file\n",
        "\n",
        "    # Convert test data to numpy arrays\n",
        "    X_test = np.array(X_test)\n",
        "    Y_test = np.array(Y_test)\n",
        "\n",
        "    # Normalize test data\n",
        "    X_test = (X_test - min_ppg) / (max_ppg - min_ppg)\n",
        "    Y_test = (Y_test - min_abp) / (max_abp - min_abp)\n",
        "\n",
        "    # Save test data split\n",
        "    with open('data/test.p', 'wb') as f:\n",
        "        pickle.dump({'X_test': X_test, 'Y_test': Y_test}, f)\n",
        "\n",
        "def main():\n",
        "    fold_data()  # Split the data for 5-fold cross-validation\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8zen_4mxFiA",
        "outputId": "d5fdde99-4c2d-4dee-a361-2baddbbc18ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Folding Data:   0%|          | 0/5 [00:00<?, ?it/s]\n",
            "Training Data Part 1:   0%|          | 0/40000 [00:00<?, ?it/s]\u001b[A\n",
            "Training Data Part 1:   7%|▋         | 2961/40000 [00:00<00:01, 29607.77it/s]\u001b[A\n",
            "Training Data Part 1:  15%|█▍        | 5964/40000 [00:00<00:01, 29855.31it/s]\u001b[A\n",
            "Training Data Part 1:  22%|██▏       | 8950/40000 [00:00<00:01, 29447.48it/s]\u001b[A\n",
            "Training Data Part 1:  30%|██▉       | 11896/40000 [00:00<00:00, 29397.93it/s]\u001b[A\n",
            "Training Data Part 1:  37%|███▋      | 14895/40000 [00:00<00:00, 29607.21it/s]\u001b[A\n",
            "Training Data Part 1:  45%|████▍     | 17857/40000 [00:00<00:00, 28117.86it/s]\u001b[A\n",
            "Training Data Part 1:  52%|█████▏    | 20796/40000 [00:00<00:00, 28515.35it/s]\u001b[A\n",
            "Training Data Part 1:  59%|█████▉    | 23658/40000 [00:00<00:00, 28368.10it/s]\u001b[A\n",
            "Training Data Part 1:  66%|██████▋   | 26560/40000 [00:00<00:00, 28565.29it/s]\u001b[A\n",
            "Training Data Part 1:  74%|███████▎  | 29422/40000 [00:01<00:00, 27937.68it/s]\u001b[A\n",
            "Training Data Part 1:  81%|████████  | 32345/40000 [00:01<00:00, 28320.17it/s]\u001b[A\n",
            "Training Data Part 1:  88%|████████▊ | 35183/40000 [00:01<00:00, 27534.20it/s]\u001b[A\n",
            "Training Data Part 1: 100%|██████████| 40000/40000 [00:01<00:00, 28534.88it/s]\n",
            "\n",
            "Training Data Part 2: 0it [00:00, ?it/s]\n",
            "\n",
            "Validation Data:   0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Data:  27%|██▋       | 2690/10000 [00:00<00:00, 26896.18it/s]\u001b[A\n",
            "Validation Data:  54%|█████▍    | 5380/10000 [00:00<00:00, 26616.63it/s]\u001b[A\n",
            "Validation Data: 100%|██████████| 10000/10000 [00:00<00:00, 27471.15it/s]\n",
            "Folding Data:  20%|██        | 1/5 [00:08<00:34,  8.54s/it]\n",
            "Training Data Part 1: 0it [00:00, ?it/s]\n",
            "\n",
            "Training Data Part 2:   0%|          | 0/40000 [00:00<?, ?it/s]\u001b[A\n",
            "Training Data Part 2:   6%|▋         | 2581/40000 [00:00<00:01, 25805.47it/s]\u001b[A\n",
            "Training Data Part 2:  14%|█▎        | 5435/40000 [00:00<00:01, 27410.50it/s]\u001b[A\n",
            "Training Data Part 2:  20%|██        | 8177/40000 [00:00<00:01, 26633.06it/s]\u001b[A\n",
            "Training Data Part 2:  27%|██▋       | 10843/40000 [00:00<00:01, 26220.14it/s]\u001b[A\n",
            "Training Data Part 2:  34%|███▍      | 13697/40000 [00:00<00:00, 27036.79it/s]\u001b[A\n",
            "Training Data Part 2:  41%|████▏     | 16514/40000 [00:00<00:00, 27413.25it/s]\u001b[A\n",
            "Training Data Part 2:  48%|████▊     | 19258/40000 [00:00<00:00, 27052.46it/s]\u001b[A\n",
            "Training Data Part 2:  55%|█████▍    | 21966/40000 [00:00<00:00, 26457.11it/s]\u001b[A\n",
            "Training Data Part 2:  62%|██████▏   | 24616/40000 [00:00<00:00, 25144.65it/s]\u001b[A\n",
            "Training Data Part 2:  68%|██████▊   | 27144/40000 [00:01<00:00, 24089.28it/s]\u001b[A\n",
            "Training Data Part 2:  75%|███████▍  | 29838/40000 [00:01<00:00, 24901.96it/s]\u001b[A\n",
            "Training Data Part 2:  82%|████████▏ | 32857/40000 [00:01<00:00, 26432.33it/s]\u001b[A\n",
            "Training Data Part 2:  89%|████████▉ | 35519/40000 [00:01<00:00, 26429.04it/s]\u001b[A\n",
            "Training Data Part 2: 100%|██████████| 40000/40000 [00:01<00:00, 26280.24it/s]\n",
            "\n",
            "Validation Data:   0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Data:  30%|██▉       | 2970/10000 [00:00<00:00, 29691.53it/s]\u001b[A\n",
            "Validation Data:  59%|█████▉    | 5940/10000 [00:00<00:00, 26734.03it/s]\u001b[A\n",
            "Validation Data: 100%|██████████| 10000/10000 [00:00<00:00, 28280.43it/s]\n",
            "Folding Data:  40%|████      | 2/5 [00:17<00:25,  8.60s/it]\n",
            "Training Data Part 1:   0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            "Training Data Part 1:  42%|████▏     | 4240/10000 [00:00<00:00, 42394.28it/s]\u001b[A\n",
            "Training Data Part 1: 100%|██████████| 10000/10000 [00:00<00:00, 43661.97it/s]\n",
            "\n",
            "Training Data Part 2:   0%|          | 0/30000 [00:00<?, ?it/s]\u001b[A\n",
            "Training Data Part 2:  16%|█▌        | 4779/30000 [00:00<00:00, 47782.18it/s]\u001b[A\n",
            "Training Data Part 2:  32%|███▏      | 9558/30000 [00:00<00:00, 40526.06it/s]\u001b[A\n",
            "Training Data Part 2:  47%|████▋     | 14131/30000 [00:00<00:00, 42686.20it/s]\u001b[A\n",
            "Training Data Part 2:  62%|██████▏   | 18459/30000 [00:00<00:00, 41572.53it/s]\u001b[A\n",
            "Training Data Part 2:  76%|███████▌  | 22652/30000 [00:00<00:00, 39692.04it/s]\u001b[A\n",
            "Training Data Part 2: 100%|██████████| 30000/30000 [00:00<00:00, 40522.72it/s]\n",
            "\n",
            "Validation Data:   0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Data:  42%|████▏     | 4157/10000 [00:00<00:00, 41567.66it/s]\u001b[A\n",
            "Validation Data: 100%|██████████| 10000/10000 [00:00<00:00, 38442.40it/s]\n",
            "Folding Data:  60%|██████    | 3/5 [00:25<00:17,  8.55s/it]\n",
            "Training Data Part 1:   0%|          | 0/20000 [00:00<?, ?it/s]\u001b[A\n",
            "Training Data Part 1:  26%|██▌       | 5143/20000 [00:00<00:00, 51426.62it/s]\u001b[A\n",
            "Training Data Part 1:  51%|█████▏    | 10286/20000 [00:00<00:00, 48966.48it/s]\u001b[A\n",
            "Training Data Part 1: 100%|██████████| 20000/20000 [00:00<00:00, 48166.65it/s]\n",
            "\n",
            "Training Data Part 2:   0%|          | 0/20000 [00:00<?, ?it/s]\u001b[A\n",
            "Training Data Part 2:  25%|██▍       | 4901/20000 [00:00<00:00, 49007.01it/s]\u001b[A\n",
            "Training Data Part 2:  49%|████▉     | 9802/20000 [00:00<00:00, 47146.28it/s]\u001b[A\n",
            "Training Data Part 2:  73%|███████▎  | 14522/20000 [00:00<00:00, 46092.72it/s]\u001b[A\n",
            "Training Data Part 2: 100%|██████████| 20000/20000 [00:00<00:00, 44220.18it/s]\n",
            "\n",
            "Validation Data:   0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Data:  36%|███▌      | 3555/10000 [00:00<00:00, 35537.66it/s]\u001b[A\n",
            "Validation Data: 100%|██████████| 10000/10000 [00:00<00:00, 37312.52it/s]\n",
            "Folding Data:  80%|████████  | 4/5 [00:31<00:07,  7.49s/it]\n",
            "Training Data Part 1:   0%|          | 0/30000 [00:00<?, ?it/s]\u001b[A\n",
            "Training Data Part 1:  12%|█▏        | 3499/30000 [00:00<00:00, 34986.78it/s]\u001b[A\n",
            "Training Data Part 1:  27%|██▋       | 8141/30000 [00:00<00:00, 41706.53it/s]\u001b[A\n",
            "Training Data Part 1:  42%|████▏     | 12488/30000 [00:00<00:00, 42508.47it/s]\u001b[A\n",
            "Training Data Part 1:  56%|█████▌    | 16739/30000 [00:00<00:00, 38814.31it/s]\u001b[A\n",
            "Training Data Part 1:  69%|██████▉   | 20664/30000 [00:00<00:00, 36376.79it/s]\u001b[A\n",
            "Training Data Part 1:  84%|████████▍ | 25280/30000 [00:00<00:00, 39425.73it/s]\u001b[A\n",
            "Training Data Part 1: 100%|██████████| 30000/30000 [00:00<00:00, 39479.69it/s]\n",
            "\n",
            "Training Data Part 2:   0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            "Training Data Part 2:  42%|████▏     | 4173/10000 [00:00<00:00, 41722.68it/s]\u001b[A\n",
            "Training Data Part 2: 100%|██████████| 10000/10000 [00:00<00:00, 40156.21it/s]\n",
            "\n",
            "Validation Data:   0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Data:  42%|████▏     | 4242/10000 [00:00<00:00, 42412.25it/s]\u001b[A\n",
            "Validation Data: 100%|██████████| 10000/10000 [00:00<00:00, 37991.23it/s]\n",
            "Folding Data: 100%|██████████| 5/5 [00:47<00:00,  9.56s/it]\n",
            "Test Data: 100%|██████████| 10000/10000 [00:00<00:00, 311087.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods to check the folds"
      ],
      "metadata": {
        "id": "_OBti0VjKI_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "def load_pickle(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "def check_files():\n",
        "    data_dir = 'data'\n",
        "    files = os.listdir(data_dir)\n",
        "\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(data_dir, file_name)\n",
        "        data = load_pickle(file_path)\n",
        "\n",
        "        print(f\"Checking {file_name}...\")\n",
        "        if 'train' in file_name or 'val' in file_name:\n",
        "            print(f\"X shape: {data['X_train'].shape if 'train' in file_name else data['X_val'].shape}\")\n",
        "            print(f\"Y shape: {data['Y_train'].shape if 'train' in file_name else data['Y_val'].shape}\")\n",
        "        elif 'test' in file_name:\n",
        "            print(f\"X shape: {data['X_test'].shape}\")\n",
        "            print(f\"Y shape: {data['Y_test'].shape}\")\n",
        "        elif 'meta' in file_name:\n",
        "            print(f\"Metadata: {data}\")\n",
        "        print()\n",
        "\n",
        "def main():\n",
        "    check_files()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw2MUx6Mxk0A",
        "outputId": "f2f5650f-b2c6-4e47-9d76-9e85cff3e7c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking test.p...\n",
            "X shape: (10000, 1250, 1)\n",
            "Y shape: (10000, 1250, 1)\n",
            "\n",
            "Checking val4.p...\n",
            "X shape: (10000, 1250, 1)\n",
            "Y shape: (10000, 1250, 1)\n",
            "\n",
            "Checking train4.p...\n",
            "X shape: (40000, 1250, 1)\n",
            "Y shape: (40000, 1250, 1)\n",
            "\n",
            "Checking meta2.p...\n",
            "Metadata: {'max_ppg': 4.001955034213099, 'min_ppg': 0.0, 'max_abp': 199.9479008990615, 'min_abp': 50.0}\n",
            "\n",
            "Checking meta1.p...\n",
            "Metadata: {'max_ppg': 4.001955034213099, 'min_ppg': 0.0, 'max_abp': 199.9479008990615, 'min_abp': 50.0}\n",
            "\n",
            "Checking train3.p...\n",
            "X shape: (40000, 1250, 1)\n",
            "Y shape: (40000, 1250, 1)\n",
            "\n",
            "Checking val1.p...\n",
            "X shape: (10000, 1250, 1)\n",
            "Y shape: (10000, 1250, 1)\n",
            "\n",
            "Checking meta0.p...\n",
            "Metadata: {'max_ppg': 4.001955034213099, 'min_ppg': 0.0, 'max_abp': 199.9479008990615, 'min_abp': 50.0}\n",
            "\n",
            "Checking val0.p...\n",
            "X shape: (10000, 1250, 1)\n",
            "Y shape: (10000, 1250, 1)\n",
            "\n",
            "Checking train2.p...\n",
            "X shape: (40000, 1250, 1)\n",
            "Y shape: (40000, 1250, 1)\n",
            "\n",
            "Checking meta3.p...\n",
            "Metadata: {'max_ppg': 4.001955034213099, 'min_ppg': 0.0, 'max_abp': 199.9479008990615, 'min_abp': 50.0}\n",
            "\n",
            "Checking meta4.p...\n",
            "Metadata: {'max_ppg': 4.001955034213099, 'min_ppg': 0.0, 'max_abp': 199.9479008990615, 'min_abp': 50.0}\n",
            "\n",
            "Checking val3.p...\n",
            "X shape: (10000, 1250, 1)\n",
            "Y shape: (10000, 1250, 1)\n",
            "\n",
            "Checking train0.p...\n",
            "X shape: (40000, 1250, 1)\n",
            "Y shape: (40000, 1250, 1)\n",
            "\n",
            "Checking train1.p...\n",
            "X shape: (40000, 1250, 1)\n",
            "Y shape: (40000, 1250, 1)\n",
            "\n",
            "Checking val2.p...\n",
            "X shape: (10000, 1250, 1)\n",
            "Y shape: (10000, 1250, 1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = '1HhcOzLbZgxOS5byOJF43r4veGWsQVb8M'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output_file = 'meta.p'\n",
        "\n",
        "gdown.download(download_url, output_file, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "gbc_b5M8Mgqi",
        "outputId": "cefbd406-5ead-462c-9839-54fb3a3851fb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HhcOzLbZgxOS5byOJF43r4veGWsQVb8M\n",
            "To: /content/meta.p\n",
            "100%|██████████| 54.0/54.0 [00:00<00:00, 188kB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'meta.p'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_pickle('meta.p')\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQim-7-ZyLl4",
        "outputId": "1f81a7ad-cc9b-41d1-f029-a0054ebebbe6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_abp': 178.8, 'min_abp': 60.2}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BP-Net architecture"
      ],
      "metadata": {
        "id": "Kgl-wSuLKTyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class IncBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, size = 15, stride = 1, padding = 7):\n",
        "        super(IncBlock,self).__init__()\n",
        "\n",
        "        self.conv1x1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias = False)\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels, out_channels//4, kernel_size = size, stride = stride, padding = padding ),\n",
        "                                   nn.BatchNorm1d(out_channels//4))\n",
        "\n",
        "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels, out_channels//4, kernel_size = 1, bias = False),\n",
        "                                   nn.BatchNorm1d(out_channels//4),\n",
        "                                   nn.LeakyReLU(0.2),\n",
        "                                   nn.Conv1d(out_channels//4, out_channels//4, kernel_size = size +2 , stride = stride, padding = padding + 1),\n",
        "                                   nn.BatchNorm1d(out_channels//4))\n",
        "\n",
        "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels, out_channels//4, kernel_size = 1, bias = False),\n",
        "                                   nn.BatchNorm1d(out_channels//4),\n",
        "                                   nn.LeakyReLU(0.2),\n",
        "                                   nn.Conv1d(out_channels//4, out_channels//4, kernel_size = size + 4 , stride = stride, padding = padding + 2),\n",
        "                                   nn.BatchNorm1d(out_channels//4))\n",
        "\n",
        "\n",
        "        self.conv4 = nn.Sequential(nn.Conv1d(in_channels, out_channels//4, kernel_size = 1, bias = False),\n",
        "                                   nn.BatchNorm1d(out_channels//4),\n",
        "                                   nn.LeakyReLU(0.2),\n",
        "                                   nn.Conv1d(out_channels//4, out_channels//4, kernel_size = size + 6 , stride = stride, padding = padding + 3),\n",
        "                                   nn.BatchNorm1d(out_channels//4))\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self,x):\n",
        "        res = self.conv1x1(x)\n",
        "#         print (res.size())\n",
        "\n",
        "\n",
        "        c1 = self.conv1(x)\n",
        "#         print (c1.size())\n",
        "\n",
        "        c2 = self.conv2(x)\n",
        "#         print (c2.size())\n",
        "\n",
        "        c3 = self.conv3(x)\n",
        "#         print (c3.size())\n",
        "\n",
        "        c4 = self.conv4(x)\n",
        "#         print (c4.size())\n",
        "\n",
        "        concat = torch.cat((c1,c2,c3,c4),dim = 1)\n",
        "\n",
        "        concat+=res\n",
        "#         print (concat.shape)\n",
        "        return self.relu(concat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InterAxialBlock(nn.Module):\n",
        "        #3\n",
        "  def __init__(self,in_channels = 1, out_channels = 1):\n",
        "\n",
        "    super(InterAxialBlock, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv1d(in_channels,8,3)\n",
        "    self.bn1 = nn.BatchNorm1d(8)\n",
        "\n",
        "    self.conv2 = nn.Conv1d(8,16,3)\n",
        "    self.bn2 =nn.BatchNorm1d(16)\n",
        "\n",
        "    self.conv3 = nn.Conv2d(1,1,(3,3), 2)\n",
        "    self.bn3 = nn.BatchNorm2d(1)\n",
        "\n",
        "    self.conv4 = nn.Conv2d(1, 1, (3,15), padding = (0,7))\n",
        "    self.bn4 = nn.BatchNorm2d(1)\n",
        "\n",
        "    self.conv5 = nn.Conv1d(1,out_channels,3, padding = 1)\n",
        "    self.bn5 = nn.BatchNorm1d(out_channels)\n",
        "    self.relu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "\n",
        "    self.mp1 = nn.MaxPool1d(2)\n",
        "    self.mp2 = nn.MaxPool2d((2,2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "#     print(\"in Inter\",x.shape)\n",
        "    x = self.relu1(self.bn1(self.conv1(x)))\n",
        "\n",
        "    x = self.relu1(self.bn2(self.conv2(x)))\n",
        "#3d -> 4d\n",
        "    x = x.view(x.shape[0],1,x.shape[1],x.shape[2])\n",
        "\n",
        "    x = self.relu1(self.bn3(self.conv3(x)))\n",
        "\n",
        "    x = self.mp2(x)\n",
        "\n",
        "\n",
        "    x = self.relu1(self.bn4(self.conv4(x)))\n",
        "\n",
        "\n",
        "    x = torch.squeeze(x, dim = 1)\n",
        "    x = self.relu1(self.bn5(self.conv5(x)))\n",
        "\n",
        "\n",
        "    return x\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, shape):\n",
        "        super(Unet, self).__init__()\n",
        "        #1\n",
        "        in_channels = 1\n",
        "\n",
        "        self.inter = nn.Sequential(InterAxialBlock())\n",
        "\n",
        "        self.en1 = nn.Sequential(nn.Conv1d(in_channels, 32, 3, padding = 1),\n",
        "                                nn.BatchNorm1d(32),\n",
        "                                nn.LeakyReLU(0.2),\n",
        "                                nn.Conv1d(32, 32, 5, stride = 2, padding = 2),\n",
        "                                IncBlock(32,32))\n",
        "\n",
        "        self.en2 = nn.Sequential(nn.Conv1d(32, 64, 3, padding = 1),\n",
        "                                nn.BatchNorm1d(64),\n",
        "                                nn.LeakyReLU(0.2),\n",
        "                                 nn.Conv1d(64, 64, 5, stride = 2, padding = 2),\n",
        "                                IncBlock(64,64))\n",
        "\n",
        "\n",
        "        self.en3 = nn.Sequential(nn.Conv1d(64,128, 3, padding = 1),\n",
        "                                 nn.BatchNorm1d(128),\n",
        "                                 nn.LeakyReLU(0.2),\n",
        "                                 nn.Conv1d(128, 128, 3, stride = 2, padding = 1),\n",
        "                                IncBlock(128,128))\n",
        "\n",
        "        self.en4 = nn.Sequential(nn.Conv1d(128,256, 3,padding = 1),\n",
        "                                 nn.BatchNorm1d(256),\n",
        "                                 nn.LeakyReLU(0.2),\n",
        "                                 nn.Conv1d(256, 256, 5, stride = 2, padding = 1),\n",
        "                                IncBlock(256,256))\n",
        "\n",
        "\n",
        "        self.en5 = nn.Sequential(nn.Conv1d(256,512, 3, padding = 1),\n",
        "                                 nn.BatchNorm1d(512),\n",
        "                                 nn.LeakyReLU(0.2),\n",
        "                                 IncBlock(512,512))\n",
        "\n",
        "\n",
        "        self.de1 = nn.Sequential(nn.ConvTranspose1d(512,256,1),\n",
        "                               nn.BatchNorm1d(256),\n",
        "                               nn.LeakyReLU(0.2),\n",
        "                                IncBlock(256,256))\n",
        "\n",
        "        self.de2 =  nn.Sequential(nn.Conv1d(512,256,3, padding = 1),\n",
        "                               nn.BatchNorm1d(256),\n",
        "                               nn.LeakyReLU(0.2),\n",
        "                                  nn.ConvTranspose1d(256,128,3, stride = 2),\n",
        "                                IncBlock(128,128))\n",
        "\n",
        "        self.de3 =  nn.Sequential(nn.Conv1d(256,128,3, stride = 1, padding = 1),\n",
        "                               nn.BatchNorm1d(128),\n",
        "                               nn.LeakyReLU(0.2),\n",
        "                                nn.ConvTranspose1d(128,64,3, stride = 2),\n",
        "                                IncBlock(64,64))\n",
        "\n",
        "        self.de4 =  nn.Sequential(nn.Conv1d(128,64,3, stride = 1, padding = 1),\n",
        "                               nn.BatchNorm1d(64),\n",
        "                               nn.LeakyReLU(0.2),\n",
        "                                nn.ConvTranspose1d(64,32,3, stride = 2),\n",
        "                                IncBlock(32,32))\n",
        "\n",
        "        self.de5 = nn.Sequential(nn.Conv1d(64,32,3, stride = 1, padding = 1),\n",
        "                               nn.BatchNorm1d(32),\n",
        "                               nn.LeakyReLU(0.2),\n",
        "                                nn.ConvTranspose1d(32,16,3, stride = 2),\n",
        "                                IncBlock(16,16))\n",
        "\n",
        "        self.de6 = nn.Sequential(nn.ConvTranspose1d(16,8,2,stride =2),\n",
        "                                nn.BatchNorm1d(8),\n",
        "                                nn.LeakyReLU(0.2))\n",
        "\n",
        "        self.de7 = nn.Sequential(nn.ConvTranspose1d(8,4,2,stride =2),\n",
        "                                nn.BatchNorm1d(4),\n",
        "                                nn.LeakyReLU(0.2))\n",
        "\n",
        "        self.de8 = nn.Sequential(nn.ConvTranspose1d(4,2,1,stride =1),\n",
        "                                nn.BatchNorm1d(2),\n",
        "                                nn.LeakyReLU(0.2))\n",
        "\n",
        "        self.de9 = nn.Sequential(nn.ConvTranspose1d(2,1,1,stride =1),\n",
        "                                nn.BatchNorm1d(1),\n",
        "                                nn.LeakyReLU(0.2))\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "#         print(\"Before inter \",x.shape)\n",
        "        x = self.inter(x)\n",
        "#         print(\" After Inter\",x.shape)\n",
        "\n",
        "        x = nn.ConstantPad1d((1,1),0)(x)\n",
        "#         print (\"After ConstantPad1d\",x.shape)\n",
        "        e1 = self.en1(x)\n",
        "#         print (\"After e1 \",e1.shape)\n",
        "\n",
        "        e2 = self.en2(e1)\n",
        "#         print (\"After e2 \",e2.shape)\n",
        "\n",
        "        e3 = self.en3(e2)\n",
        "#         print (\"After e3 \",e3.shape)\n",
        "\n",
        "        e4 = self.en4(e3)\n",
        "#         print (\"After e4  \",e4.shape)\n",
        "\n",
        "        e5 = self.en5(e4)\n",
        "#         print (\"After e5 \",e5.shape)\n",
        "#         print (\"-----------------------------------------------------------------------------\")\n",
        "        d1 = self.de1(e5)\n",
        "#         print (\"After d1\", d1.shape)\n",
        "\n",
        "#         print(\"Before cat d1 e4 {} {}\".format(d1.shape,e4.shape))\n",
        "        cat = torch.cat([d1,e4],1)\n",
        "#         print(\"After cat d1 e4 {}\".format(cat.shape))\n",
        "\n",
        "        d2 = self.de2(cat)\n",
        "#         print (\"After d2 \",d2.shape)\n",
        "\n",
        "#         print (\"Before cat d2 e3 {} {}  \".format(d2.shape,e3.shape))\n",
        "        cat = torch.cat([d2,e3[:,:,:-1]],1)\n",
        "#         print(\"After cat d2 e3 {}\".format(cat.shape))\n",
        "\n",
        "\n",
        "\n",
        "        d3 = self.de3(cat)\n",
        "\n",
        "#         print (\"After d3 \",d3.shape)\n",
        "#         print (\"Before cat d3 e2 {} {}  \".format(d3.shape,e2.shape))\n",
        "#         print(\"-1 being done on d3\")\n",
        "        cat = torch.cat([d3,e2[:,:,:]],1) #MADE A CHANGE HERE, ADDED -1\n",
        "#         print(\"After cat d3 e2 {}\".format(cat.shape))\n",
        "\n",
        "        d4 = self.de4(cat)\n",
        "#         print (\"After d4 \",d4.shape)\n",
        "\n",
        "#         print (\"Before cat d4 e1 {} {}  \".format(d4.shape,e1.shape))\n",
        "        cat = torch.cat([d4[:,:,:-2],e1],1) #MADE A CHANGE HERE, ([d4[:,:,:-2],e1],1) this is the original one\n",
        "#         print(\"After cat d4 e1 {}\".format(cat.shape))\n",
        "\n",
        "        d5 = self.de5(cat)[:,:,:-2]\n",
        "#         print (\"After d5 \", d5.shape)\n",
        "\n",
        "        d6 = self.de6(d5)[:,:,:-1]\n",
        "\n",
        "#         print(d6.shape)\n",
        "\n",
        "        d7 = self.de7(d6)\n",
        "#         print(\"d7 \", d7.shape)\n",
        "        d8 = self.de8(d7)\n",
        "#         print(d8.shape)\n",
        "        d9 = self.de9(d8)\n",
        "#         print(d9.shape)\n",
        "        return d9"
      ],
      "metadata": {
        "id": "BlC0hKEEy6-F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading:\n",
        "\n",
        "BPdatasetv1 used for SSL\n",
        "\n",
        "BPdatasetv2 used for training"
      ],
      "metadata": {
        "id": "l5jV7ZOUKaiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BPdatasetv1(Dataset):\n",
        "    def __init__(self, fold_num, train=False, val=False):\n",
        "        if train:\n",
        "            dt = pickle.load(open(os.path.join('data', f'train{fold_num}.p'), 'rb'))\n",
        "            self.input = np.swapaxes(dt['X_train'], 1, 2).astype('float32')\n",
        "            self.output = np.swapaxes(dt['X_train'], 1, 2).astype('float32')\n",
        "        elif val:\n",
        "            dt = pickle.load(open(os.path.join('data', f'val{fold_num}.p'), 'rb'))\n",
        "            self.input = np.swapaxes(dt['X_val'], 1, 2).astype('float32')\n",
        "            self.output = np.swapaxes(dt['X_val'], 1, 2).astype('float32')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp = self.input[idx]\n",
        "        out = self.output[idx]\n",
        "        return inp, out\n",
        "\n",
        "class BPdatasetv2(Dataset):\n",
        "    def __init__(self, fold_num, train=False, val=False, test=False):\n",
        "        if train:\n",
        "            dt = pickle.load(open(os.path.join('data', f'train{fold_num}.p'), 'rb'))\n",
        "            self.input = np.swapaxes(dt['X_train'], 1, 2).astype('float32')\n",
        "            self.output = np.swapaxes(dt['Y_train'], 1, 2).astype('float32')\n",
        "        elif val:\n",
        "            dt = pickle.load(open(os.path.join('data', f'val{fold_num}.p'), 'rb'))\n",
        "            self.input = np.swapaxes(dt['X_val'], 1, 2).astype('float32')\n",
        "            self.output = np.swapaxes(dt['Y_val'], 1, 2).astype('float32')\n",
        "        elif test:\n",
        "            dt = pickle.load(open(os.path.join('data', 'test.p'), 'rb'))\n",
        "            self.input = np.swapaxes(dt['X_test'], 1, 2).astype('float32')\n",
        "            self.output = np.swapaxes(dt['Y_test'], 1, 2).astype('float32')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp = self.input[idx]\n",
        "        out = self.output[idx]\n",
        "        return inp, out\n"
      ],
      "metadata": {
        "id": "piJnvHykzEo7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EarlyStopper class to improve performance and discard unnecessary computations"
      ],
      "metadata": {
        "id": "BGIzhjO9KqKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=3, min_delta=10):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = float('inf')\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "789A53Ba0dPY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model"
      ],
      "metadata": {
        "id": "WvL7fIe30szj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-supervision (supervision)"
      ],
      "metadata": {
        "id": "QdbOL4cLK0yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "bs = 256\n",
        "length = 1250\n",
        "epochs = 20\n",
        "folds = 5\n",
        "\n",
        "model = Unet((bs, 1, length)).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = torch.nn.MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 200], gamma=0.1)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "early_stopper = EarlyStopper(patience=7, min_delta=0)\n",
        "\n",
        "best_loss = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_loss_v = 0.0\n",
        "\n",
        "    for fold in range(1, folds + 1):\n",
        "        train_loader = DataLoader(BPdatasetv1(fold-1, train=True), batch_size=bs, shuffle=True)\n",
        "        val_loader = DataLoader(BPdatasetv1(fold-1, val=True), batch_size=bs, shuffle=False)\n",
        "\n",
        "        for idx, (inputs, output) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "            inputs = inputs.cuda()\n",
        "            output = output.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                pred = model(inputs)\n",
        "                loss = criterion(pred, output)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # VALIDATION\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for idx, (inputs_v, labels_v) in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
        "                inputs_v = inputs_v.cuda()\n",
        "                labels_v = labels_v.cuda()\n",
        "                outputs_v = model(inputs_v).cuda()\n",
        "                loss_v = criterion(outputs_v, labels_v)\n",
        "                running_loss_v += loss_v.item() * inputs_v.size(0)\n",
        "\n",
        "    avg_train_loss = running_loss / (len(train_loader.dataset) * folds)\n",
        "    avg_val_loss = running_loss_v / (len(val_loader.dataset) * folds)\n",
        "\n",
        "    path = 'model/ssl.pt'\n",
        "\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'best_dev_loss': best_loss,\n",
        "            'exp_dir': 'model'\n",
        "        }, f=path)\n",
        "    print('Loss: {:.4f}   Val_loss: {:.4f}'.format(avg_train_loss, avg_val_loss))\n",
        "\n",
        "    if early_stopper.early_stop(avg_val_loss):\n",
        "        print(\"Early stopping\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD4o59ZA0gR_",
        "outputId": "ed5be462-6959-4426-b80e-53b773381a90"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.09it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.65it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.33it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.15it/s]\n",
            "100%|██████████| 157/157 [00:14<00:00, 11.15it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.31it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.33it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.30it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.40it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.1175   Val_loss: 0.0591\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 10.78it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.29it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.32it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.08it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.26it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.57it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.51it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.56it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.45it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0376   Val_loss: 0.0249\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.12it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.41it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.40it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.50it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.22it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.73it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.35it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.46it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.42it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0189   Val_loss: 0.0149\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 10.87it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.50it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.34it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.80it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.45it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.52it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.55it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.56it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0134   Val_loss: 0.0123\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.14it/s]\n",
            "100%|██████████| 40/40 [00:02<00:00, 18.84it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.51it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.63it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.65it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.58it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.29it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.25it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.56it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0117   Val_loss: 0.0102\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.34it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.57it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.61it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.67it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.33it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.54it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.41it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.61it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0112   Val_loss: 0.0089\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.32it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.52it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.06it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.51it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.59it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.55it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.49it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.65it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0101   Val_loss: 0.0094\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.33it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.19it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.52it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.45it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.54it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.61it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.67it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.63it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0093   Val_loss: 0.0091\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.02it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.49it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.57it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.62it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.72it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.73it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.62it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.45it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0083   Val_loss: 0.0085\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.26it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.44it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.59it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.53it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.43it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.43it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.05it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.62it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0076   Val_loss: 0.0081\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.24it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.67it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.65it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.51it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.50it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.19it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.64it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.60it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.60it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0069   Val_loss: 0.0004\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.25it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.43it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.18it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.62it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.56it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.71it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.61it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0067   Val_loss: 0.0163\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.18it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.91it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.68it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.57it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.57it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.67it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.71it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.50it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0066   Val_loss: 0.0046\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.32it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.65it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.59it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.62it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.64it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.57it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.87it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0055   Val_loss: 0.0042\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.18it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.64it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.65it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.58it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.63it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.15it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.43it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.75it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.45it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0049   Val_loss: 0.0035\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.19it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.48it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.37it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.15it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.46it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.62it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.49it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.59it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.44it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0046   Val_loss: 0.0034\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.22it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.84it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.41it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.50it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.47it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.60it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.49it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.64it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0043   Val_loss: 0.0042\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.17it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.58it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.62it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.63it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.57it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.62it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.88it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.53it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0043   Val_loss: 0.0081\n",
            "Early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the model after SSL"
      ],
      "metadata": {
        "id": "GjlvRlWrK7lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the path to your checkpoint file\n",
        "path = 'model/ssl.pt'\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(path)\n",
        "\n",
        "# Extract components from the checkpoint\n",
        "epoch = checkpoint['epoch']  # Epoch number when checkpoint was saved\n",
        "model_state_dict = checkpoint['model']  # State dictionary of the model\n",
        "optimizer_state_dict = checkpoint['optimizer']  # State dictionary of the optimizer\n",
        "best_dev_loss = checkpoint['best_dev_loss']  # Best validation loss recorded\n",
        "exp_dir = checkpoint['exp_dir']  # Directory or other metadata related to the experiment\n",
        "\n",
        "# Print or inspect these components as needed\n",
        "print(f'Epoch: {epoch}')\n",
        "print(f'Best Validation Loss: {best_dev_loss}')\n",
        "print(f'Experiment Directory: {exp_dir}')\n",
        "\n",
        "# Example of inspecting the model state dictionary keys\n",
        "print('Model State Dictionary Keys:')\n",
        "for key in model_state_dict.keys():\n",
        "    print(key)\n",
        "\n",
        "# Example of inspecting optimizer state dictionary keys\n",
        "print('Optimizer State Dictionary Keys:')\n",
        "for key in optimizer_state_dict.keys():\n",
        "    print(key)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Koghd7Xt-h4J",
        "outputId": "80e46259-38b5-44b5-b7fa-e73f752ab351"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10\n",
            "Best Validation Loss: 0.00039534185132943095\n",
            "Experiment Directory: model\n",
            "Model State Dictionary Keys:\n",
            "inter.0.conv1.weight\n",
            "inter.0.conv1.bias\n",
            "inter.0.bn1.weight\n",
            "inter.0.bn1.bias\n",
            "inter.0.bn1.running_mean\n",
            "inter.0.bn1.running_var\n",
            "inter.0.bn1.num_batches_tracked\n",
            "inter.0.conv2.weight\n",
            "inter.0.conv2.bias\n",
            "inter.0.bn2.weight\n",
            "inter.0.bn2.bias\n",
            "inter.0.bn2.running_mean\n",
            "inter.0.bn2.running_var\n",
            "inter.0.bn2.num_batches_tracked\n",
            "inter.0.conv3.weight\n",
            "inter.0.conv3.bias\n",
            "inter.0.bn3.weight\n",
            "inter.0.bn3.bias\n",
            "inter.0.bn3.running_mean\n",
            "inter.0.bn3.running_var\n",
            "inter.0.bn3.num_batches_tracked\n",
            "inter.0.conv4.weight\n",
            "inter.0.conv4.bias\n",
            "inter.0.bn4.weight\n",
            "inter.0.bn4.bias\n",
            "inter.0.bn4.running_mean\n",
            "inter.0.bn4.running_var\n",
            "inter.0.bn4.num_batches_tracked\n",
            "inter.0.conv5.weight\n",
            "inter.0.conv5.bias\n",
            "inter.0.bn5.weight\n",
            "inter.0.bn5.bias\n",
            "inter.0.bn5.running_mean\n",
            "inter.0.bn5.running_var\n",
            "inter.0.bn5.num_batches_tracked\n",
            "en1.0.weight\n",
            "en1.0.bias\n",
            "en1.1.weight\n",
            "en1.1.bias\n",
            "en1.1.running_mean\n",
            "en1.1.running_var\n",
            "en1.1.num_batches_tracked\n",
            "en1.3.weight\n",
            "en1.3.bias\n",
            "en1.4.conv1x1.weight\n",
            "en1.4.conv1.0.weight\n",
            "en1.4.conv1.0.bias\n",
            "en1.4.conv1.1.weight\n",
            "en1.4.conv1.1.bias\n",
            "en1.4.conv1.1.running_mean\n",
            "en1.4.conv1.1.running_var\n",
            "en1.4.conv1.1.num_batches_tracked\n",
            "en1.4.conv2.0.weight\n",
            "en1.4.conv2.1.weight\n",
            "en1.4.conv2.1.bias\n",
            "en1.4.conv2.1.running_mean\n",
            "en1.4.conv2.1.running_var\n",
            "en1.4.conv2.1.num_batches_tracked\n",
            "en1.4.conv2.3.weight\n",
            "en1.4.conv2.3.bias\n",
            "en1.4.conv2.4.weight\n",
            "en1.4.conv2.4.bias\n",
            "en1.4.conv2.4.running_mean\n",
            "en1.4.conv2.4.running_var\n",
            "en1.4.conv2.4.num_batches_tracked\n",
            "en1.4.conv3.0.weight\n",
            "en1.4.conv3.1.weight\n",
            "en1.4.conv3.1.bias\n",
            "en1.4.conv3.1.running_mean\n",
            "en1.4.conv3.1.running_var\n",
            "en1.4.conv3.1.num_batches_tracked\n",
            "en1.4.conv3.3.weight\n",
            "en1.4.conv3.3.bias\n",
            "en1.4.conv3.4.weight\n",
            "en1.4.conv3.4.bias\n",
            "en1.4.conv3.4.running_mean\n",
            "en1.4.conv3.4.running_var\n",
            "en1.4.conv3.4.num_batches_tracked\n",
            "en1.4.conv4.0.weight\n",
            "en1.4.conv4.1.weight\n",
            "en1.4.conv4.1.bias\n",
            "en1.4.conv4.1.running_mean\n",
            "en1.4.conv4.1.running_var\n",
            "en1.4.conv4.1.num_batches_tracked\n",
            "en1.4.conv4.3.weight\n",
            "en1.4.conv4.3.bias\n",
            "en1.4.conv4.4.weight\n",
            "en1.4.conv4.4.bias\n",
            "en1.4.conv4.4.running_mean\n",
            "en1.4.conv4.4.running_var\n",
            "en1.4.conv4.4.num_batches_tracked\n",
            "en2.0.weight\n",
            "en2.0.bias\n",
            "en2.1.weight\n",
            "en2.1.bias\n",
            "en2.1.running_mean\n",
            "en2.1.running_var\n",
            "en2.1.num_batches_tracked\n",
            "en2.3.weight\n",
            "en2.3.bias\n",
            "en2.4.conv1x1.weight\n",
            "en2.4.conv1.0.weight\n",
            "en2.4.conv1.0.bias\n",
            "en2.4.conv1.1.weight\n",
            "en2.4.conv1.1.bias\n",
            "en2.4.conv1.1.running_mean\n",
            "en2.4.conv1.1.running_var\n",
            "en2.4.conv1.1.num_batches_tracked\n",
            "en2.4.conv2.0.weight\n",
            "en2.4.conv2.1.weight\n",
            "en2.4.conv2.1.bias\n",
            "en2.4.conv2.1.running_mean\n",
            "en2.4.conv2.1.running_var\n",
            "en2.4.conv2.1.num_batches_tracked\n",
            "en2.4.conv2.3.weight\n",
            "en2.4.conv2.3.bias\n",
            "en2.4.conv2.4.weight\n",
            "en2.4.conv2.4.bias\n",
            "en2.4.conv2.4.running_mean\n",
            "en2.4.conv2.4.running_var\n",
            "en2.4.conv2.4.num_batches_tracked\n",
            "en2.4.conv3.0.weight\n",
            "en2.4.conv3.1.weight\n",
            "en2.4.conv3.1.bias\n",
            "en2.4.conv3.1.running_mean\n",
            "en2.4.conv3.1.running_var\n",
            "en2.4.conv3.1.num_batches_tracked\n",
            "en2.4.conv3.3.weight\n",
            "en2.4.conv3.3.bias\n",
            "en2.4.conv3.4.weight\n",
            "en2.4.conv3.4.bias\n",
            "en2.4.conv3.4.running_mean\n",
            "en2.4.conv3.4.running_var\n",
            "en2.4.conv3.4.num_batches_tracked\n",
            "en2.4.conv4.0.weight\n",
            "en2.4.conv4.1.weight\n",
            "en2.4.conv4.1.bias\n",
            "en2.4.conv4.1.running_mean\n",
            "en2.4.conv4.1.running_var\n",
            "en2.4.conv4.1.num_batches_tracked\n",
            "en2.4.conv4.3.weight\n",
            "en2.4.conv4.3.bias\n",
            "en2.4.conv4.4.weight\n",
            "en2.4.conv4.4.bias\n",
            "en2.4.conv4.4.running_mean\n",
            "en2.4.conv4.4.running_var\n",
            "en2.4.conv4.4.num_batches_tracked\n",
            "en3.0.weight\n",
            "en3.0.bias\n",
            "en3.1.weight\n",
            "en3.1.bias\n",
            "en3.1.running_mean\n",
            "en3.1.running_var\n",
            "en3.1.num_batches_tracked\n",
            "en3.3.weight\n",
            "en3.3.bias\n",
            "en3.4.conv1x1.weight\n",
            "en3.4.conv1.0.weight\n",
            "en3.4.conv1.0.bias\n",
            "en3.4.conv1.1.weight\n",
            "en3.4.conv1.1.bias\n",
            "en3.4.conv1.1.running_mean\n",
            "en3.4.conv1.1.running_var\n",
            "en3.4.conv1.1.num_batches_tracked\n",
            "en3.4.conv2.0.weight\n",
            "en3.4.conv2.1.weight\n",
            "en3.4.conv2.1.bias\n",
            "en3.4.conv2.1.running_mean\n",
            "en3.4.conv2.1.running_var\n",
            "en3.4.conv2.1.num_batches_tracked\n",
            "en3.4.conv2.3.weight\n",
            "en3.4.conv2.3.bias\n",
            "en3.4.conv2.4.weight\n",
            "en3.4.conv2.4.bias\n",
            "en3.4.conv2.4.running_mean\n",
            "en3.4.conv2.4.running_var\n",
            "en3.4.conv2.4.num_batches_tracked\n",
            "en3.4.conv3.0.weight\n",
            "en3.4.conv3.1.weight\n",
            "en3.4.conv3.1.bias\n",
            "en3.4.conv3.1.running_mean\n",
            "en3.4.conv3.1.running_var\n",
            "en3.4.conv3.1.num_batches_tracked\n",
            "en3.4.conv3.3.weight\n",
            "en3.4.conv3.3.bias\n",
            "en3.4.conv3.4.weight\n",
            "en3.4.conv3.4.bias\n",
            "en3.4.conv3.4.running_mean\n",
            "en3.4.conv3.4.running_var\n",
            "en3.4.conv3.4.num_batches_tracked\n",
            "en3.4.conv4.0.weight\n",
            "en3.4.conv4.1.weight\n",
            "en3.4.conv4.1.bias\n",
            "en3.4.conv4.1.running_mean\n",
            "en3.4.conv4.1.running_var\n",
            "en3.4.conv4.1.num_batches_tracked\n",
            "en3.4.conv4.3.weight\n",
            "en3.4.conv4.3.bias\n",
            "en3.4.conv4.4.weight\n",
            "en3.4.conv4.4.bias\n",
            "en3.4.conv4.4.running_mean\n",
            "en3.4.conv4.4.running_var\n",
            "en3.4.conv4.4.num_batches_tracked\n",
            "en4.0.weight\n",
            "en4.0.bias\n",
            "en4.1.weight\n",
            "en4.1.bias\n",
            "en4.1.running_mean\n",
            "en4.1.running_var\n",
            "en4.1.num_batches_tracked\n",
            "en4.3.weight\n",
            "en4.3.bias\n",
            "en4.4.conv1x1.weight\n",
            "en4.4.conv1.0.weight\n",
            "en4.4.conv1.0.bias\n",
            "en4.4.conv1.1.weight\n",
            "en4.4.conv1.1.bias\n",
            "en4.4.conv1.1.running_mean\n",
            "en4.4.conv1.1.running_var\n",
            "en4.4.conv1.1.num_batches_tracked\n",
            "en4.4.conv2.0.weight\n",
            "en4.4.conv2.1.weight\n",
            "en4.4.conv2.1.bias\n",
            "en4.4.conv2.1.running_mean\n",
            "en4.4.conv2.1.running_var\n",
            "en4.4.conv2.1.num_batches_tracked\n",
            "en4.4.conv2.3.weight\n",
            "en4.4.conv2.3.bias\n",
            "en4.4.conv2.4.weight\n",
            "en4.4.conv2.4.bias\n",
            "en4.4.conv2.4.running_mean\n",
            "en4.4.conv2.4.running_var\n",
            "en4.4.conv2.4.num_batches_tracked\n",
            "en4.4.conv3.0.weight\n",
            "en4.4.conv3.1.weight\n",
            "en4.4.conv3.1.bias\n",
            "en4.4.conv3.1.running_mean\n",
            "en4.4.conv3.1.running_var\n",
            "en4.4.conv3.1.num_batches_tracked\n",
            "en4.4.conv3.3.weight\n",
            "en4.4.conv3.3.bias\n",
            "en4.4.conv3.4.weight\n",
            "en4.4.conv3.4.bias\n",
            "en4.4.conv3.4.running_mean\n",
            "en4.4.conv3.4.running_var\n",
            "en4.4.conv3.4.num_batches_tracked\n",
            "en4.4.conv4.0.weight\n",
            "en4.4.conv4.1.weight\n",
            "en4.4.conv4.1.bias\n",
            "en4.4.conv4.1.running_mean\n",
            "en4.4.conv4.1.running_var\n",
            "en4.4.conv4.1.num_batches_tracked\n",
            "en4.4.conv4.3.weight\n",
            "en4.4.conv4.3.bias\n",
            "en4.4.conv4.4.weight\n",
            "en4.4.conv4.4.bias\n",
            "en4.4.conv4.4.running_mean\n",
            "en4.4.conv4.4.running_var\n",
            "en4.4.conv4.4.num_batches_tracked\n",
            "en5.0.weight\n",
            "en5.0.bias\n",
            "en5.1.weight\n",
            "en5.1.bias\n",
            "en5.1.running_mean\n",
            "en5.1.running_var\n",
            "en5.1.num_batches_tracked\n",
            "en5.3.conv1x1.weight\n",
            "en5.3.conv1.0.weight\n",
            "en5.3.conv1.0.bias\n",
            "en5.3.conv1.1.weight\n",
            "en5.3.conv1.1.bias\n",
            "en5.3.conv1.1.running_mean\n",
            "en5.3.conv1.1.running_var\n",
            "en5.3.conv1.1.num_batches_tracked\n",
            "en5.3.conv2.0.weight\n",
            "en5.3.conv2.1.weight\n",
            "en5.3.conv2.1.bias\n",
            "en5.3.conv2.1.running_mean\n",
            "en5.3.conv2.1.running_var\n",
            "en5.3.conv2.1.num_batches_tracked\n",
            "en5.3.conv2.3.weight\n",
            "en5.3.conv2.3.bias\n",
            "en5.3.conv2.4.weight\n",
            "en5.3.conv2.4.bias\n",
            "en5.3.conv2.4.running_mean\n",
            "en5.3.conv2.4.running_var\n",
            "en5.3.conv2.4.num_batches_tracked\n",
            "en5.3.conv3.0.weight\n",
            "en5.3.conv3.1.weight\n",
            "en5.3.conv3.1.bias\n",
            "en5.3.conv3.1.running_mean\n",
            "en5.3.conv3.1.running_var\n",
            "en5.3.conv3.1.num_batches_tracked\n",
            "en5.3.conv3.3.weight\n",
            "en5.3.conv3.3.bias\n",
            "en5.3.conv3.4.weight\n",
            "en5.3.conv3.4.bias\n",
            "en5.3.conv3.4.running_mean\n",
            "en5.3.conv3.4.running_var\n",
            "en5.3.conv3.4.num_batches_tracked\n",
            "en5.3.conv4.0.weight\n",
            "en5.3.conv4.1.weight\n",
            "en5.3.conv4.1.bias\n",
            "en5.3.conv4.1.running_mean\n",
            "en5.3.conv4.1.running_var\n",
            "en5.3.conv4.1.num_batches_tracked\n",
            "en5.3.conv4.3.weight\n",
            "en5.3.conv4.3.bias\n",
            "en5.3.conv4.4.weight\n",
            "en5.3.conv4.4.bias\n",
            "en5.3.conv4.4.running_mean\n",
            "en5.3.conv4.4.running_var\n",
            "en5.3.conv4.4.num_batches_tracked\n",
            "de1.0.weight\n",
            "de1.0.bias\n",
            "de1.1.weight\n",
            "de1.1.bias\n",
            "de1.1.running_mean\n",
            "de1.1.running_var\n",
            "de1.1.num_batches_tracked\n",
            "de1.3.conv1x1.weight\n",
            "de1.3.conv1.0.weight\n",
            "de1.3.conv1.0.bias\n",
            "de1.3.conv1.1.weight\n",
            "de1.3.conv1.1.bias\n",
            "de1.3.conv1.1.running_mean\n",
            "de1.3.conv1.1.running_var\n",
            "de1.3.conv1.1.num_batches_tracked\n",
            "de1.3.conv2.0.weight\n",
            "de1.3.conv2.1.weight\n",
            "de1.3.conv2.1.bias\n",
            "de1.3.conv2.1.running_mean\n",
            "de1.3.conv2.1.running_var\n",
            "de1.3.conv2.1.num_batches_tracked\n",
            "de1.3.conv2.3.weight\n",
            "de1.3.conv2.3.bias\n",
            "de1.3.conv2.4.weight\n",
            "de1.3.conv2.4.bias\n",
            "de1.3.conv2.4.running_mean\n",
            "de1.3.conv2.4.running_var\n",
            "de1.3.conv2.4.num_batches_tracked\n",
            "de1.3.conv3.0.weight\n",
            "de1.3.conv3.1.weight\n",
            "de1.3.conv3.1.bias\n",
            "de1.3.conv3.1.running_mean\n",
            "de1.3.conv3.1.running_var\n",
            "de1.3.conv3.1.num_batches_tracked\n",
            "de1.3.conv3.3.weight\n",
            "de1.3.conv3.3.bias\n",
            "de1.3.conv3.4.weight\n",
            "de1.3.conv3.4.bias\n",
            "de1.3.conv3.4.running_mean\n",
            "de1.3.conv3.4.running_var\n",
            "de1.3.conv3.4.num_batches_tracked\n",
            "de1.3.conv4.0.weight\n",
            "de1.3.conv4.1.weight\n",
            "de1.3.conv4.1.bias\n",
            "de1.3.conv4.1.running_mean\n",
            "de1.3.conv4.1.running_var\n",
            "de1.3.conv4.1.num_batches_tracked\n",
            "de1.3.conv4.3.weight\n",
            "de1.3.conv4.3.bias\n",
            "de1.3.conv4.4.weight\n",
            "de1.3.conv4.4.bias\n",
            "de1.3.conv4.4.running_mean\n",
            "de1.3.conv4.4.running_var\n",
            "de1.3.conv4.4.num_batches_tracked\n",
            "de2.0.weight\n",
            "de2.0.bias\n",
            "de2.1.weight\n",
            "de2.1.bias\n",
            "de2.1.running_mean\n",
            "de2.1.running_var\n",
            "de2.1.num_batches_tracked\n",
            "de2.3.weight\n",
            "de2.3.bias\n",
            "de2.4.conv1x1.weight\n",
            "de2.4.conv1.0.weight\n",
            "de2.4.conv1.0.bias\n",
            "de2.4.conv1.1.weight\n",
            "de2.4.conv1.1.bias\n",
            "de2.4.conv1.1.running_mean\n",
            "de2.4.conv1.1.running_var\n",
            "de2.4.conv1.1.num_batches_tracked\n",
            "de2.4.conv2.0.weight\n",
            "de2.4.conv2.1.weight\n",
            "de2.4.conv2.1.bias\n",
            "de2.4.conv2.1.running_mean\n",
            "de2.4.conv2.1.running_var\n",
            "de2.4.conv2.1.num_batches_tracked\n",
            "de2.4.conv2.3.weight\n",
            "de2.4.conv2.3.bias\n",
            "de2.4.conv2.4.weight\n",
            "de2.4.conv2.4.bias\n",
            "de2.4.conv2.4.running_mean\n",
            "de2.4.conv2.4.running_var\n",
            "de2.4.conv2.4.num_batches_tracked\n",
            "de2.4.conv3.0.weight\n",
            "de2.4.conv3.1.weight\n",
            "de2.4.conv3.1.bias\n",
            "de2.4.conv3.1.running_mean\n",
            "de2.4.conv3.1.running_var\n",
            "de2.4.conv3.1.num_batches_tracked\n",
            "de2.4.conv3.3.weight\n",
            "de2.4.conv3.3.bias\n",
            "de2.4.conv3.4.weight\n",
            "de2.4.conv3.4.bias\n",
            "de2.4.conv3.4.running_mean\n",
            "de2.4.conv3.4.running_var\n",
            "de2.4.conv3.4.num_batches_tracked\n",
            "de2.4.conv4.0.weight\n",
            "de2.4.conv4.1.weight\n",
            "de2.4.conv4.1.bias\n",
            "de2.4.conv4.1.running_mean\n",
            "de2.4.conv4.1.running_var\n",
            "de2.4.conv4.1.num_batches_tracked\n",
            "de2.4.conv4.3.weight\n",
            "de2.4.conv4.3.bias\n",
            "de2.4.conv4.4.weight\n",
            "de2.4.conv4.4.bias\n",
            "de2.4.conv4.4.running_mean\n",
            "de2.4.conv4.4.running_var\n",
            "de2.4.conv4.4.num_batches_tracked\n",
            "de3.0.weight\n",
            "de3.0.bias\n",
            "de3.1.weight\n",
            "de3.1.bias\n",
            "de3.1.running_mean\n",
            "de3.1.running_var\n",
            "de3.1.num_batches_tracked\n",
            "de3.3.weight\n",
            "de3.3.bias\n",
            "de3.4.conv1x1.weight\n",
            "de3.4.conv1.0.weight\n",
            "de3.4.conv1.0.bias\n",
            "de3.4.conv1.1.weight\n",
            "de3.4.conv1.1.bias\n",
            "de3.4.conv1.1.running_mean\n",
            "de3.4.conv1.1.running_var\n",
            "de3.4.conv1.1.num_batches_tracked\n",
            "de3.4.conv2.0.weight\n",
            "de3.4.conv2.1.weight\n",
            "de3.4.conv2.1.bias\n",
            "de3.4.conv2.1.running_mean\n",
            "de3.4.conv2.1.running_var\n",
            "de3.4.conv2.1.num_batches_tracked\n",
            "de3.4.conv2.3.weight\n",
            "de3.4.conv2.3.bias\n",
            "de3.4.conv2.4.weight\n",
            "de3.4.conv2.4.bias\n",
            "de3.4.conv2.4.running_mean\n",
            "de3.4.conv2.4.running_var\n",
            "de3.4.conv2.4.num_batches_tracked\n",
            "de3.4.conv3.0.weight\n",
            "de3.4.conv3.1.weight\n",
            "de3.4.conv3.1.bias\n",
            "de3.4.conv3.1.running_mean\n",
            "de3.4.conv3.1.running_var\n",
            "de3.4.conv3.1.num_batches_tracked\n",
            "de3.4.conv3.3.weight\n",
            "de3.4.conv3.3.bias\n",
            "de3.4.conv3.4.weight\n",
            "de3.4.conv3.4.bias\n",
            "de3.4.conv3.4.running_mean\n",
            "de3.4.conv3.4.running_var\n",
            "de3.4.conv3.4.num_batches_tracked\n",
            "de3.4.conv4.0.weight\n",
            "de3.4.conv4.1.weight\n",
            "de3.4.conv4.1.bias\n",
            "de3.4.conv4.1.running_mean\n",
            "de3.4.conv4.1.running_var\n",
            "de3.4.conv4.1.num_batches_tracked\n",
            "de3.4.conv4.3.weight\n",
            "de3.4.conv4.3.bias\n",
            "de3.4.conv4.4.weight\n",
            "de3.4.conv4.4.bias\n",
            "de3.4.conv4.4.running_mean\n",
            "de3.4.conv4.4.running_var\n",
            "de3.4.conv4.4.num_batches_tracked\n",
            "de4.0.weight\n",
            "de4.0.bias\n",
            "de4.1.weight\n",
            "de4.1.bias\n",
            "de4.1.running_mean\n",
            "de4.1.running_var\n",
            "de4.1.num_batches_tracked\n",
            "de4.3.weight\n",
            "de4.3.bias\n",
            "de4.4.conv1x1.weight\n",
            "de4.4.conv1.0.weight\n",
            "de4.4.conv1.0.bias\n",
            "de4.4.conv1.1.weight\n",
            "de4.4.conv1.1.bias\n",
            "de4.4.conv1.1.running_mean\n",
            "de4.4.conv1.1.running_var\n",
            "de4.4.conv1.1.num_batches_tracked\n",
            "de4.4.conv2.0.weight\n",
            "de4.4.conv2.1.weight\n",
            "de4.4.conv2.1.bias\n",
            "de4.4.conv2.1.running_mean\n",
            "de4.4.conv2.1.running_var\n",
            "de4.4.conv2.1.num_batches_tracked\n",
            "de4.4.conv2.3.weight\n",
            "de4.4.conv2.3.bias\n",
            "de4.4.conv2.4.weight\n",
            "de4.4.conv2.4.bias\n",
            "de4.4.conv2.4.running_mean\n",
            "de4.4.conv2.4.running_var\n",
            "de4.4.conv2.4.num_batches_tracked\n",
            "de4.4.conv3.0.weight\n",
            "de4.4.conv3.1.weight\n",
            "de4.4.conv3.1.bias\n",
            "de4.4.conv3.1.running_mean\n",
            "de4.4.conv3.1.running_var\n",
            "de4.4.conv3.1.num_batches_tracked\n",
            "de4.4.conv3.3.weight\n",
            "de4.4.conv3.3.bias\n",
            "de4.4.conv3.4.weight\n",
            "de4.4.conv3.4.bias\n",
            "de4.4.conv3.4.running_mean\n",
            "de4.4.conv3.4.running_var\n",
            "de4.4.conv3.4.num_batches_tracked\n",
            "de4.4.conv4.0.weight\n",
            "de4.4.conv4.1.weight\n",
            "de4.4.conv4.1.bias\n",
            "de4.4.conv4.1.running_mean\n",
            "de4.4.conv4.1.running_var\n",
            "de4.4.conv4.1.num_batches_tracked\n",
            "de4.4.conv4.3.weight\n",
            "de4.4.conv4.3.bias\n",
            "de4.4.conv4.4.weight\n",
            "de4.4.conv4.4.bias\n",
            "de4.4.conv4.4.running_mean\n",
            "de4.4.conv4.4.running_var\n",
            "de4.4.conv4.4.num_batches_tracked\n",
            "de5.0.weight\n",
            "de5.0.bias\n",
            "de5.1.weight\n",
            "de5.1.bias\n",
            "de5.1.running_mean\n",
            "de5.1.running_var\n",
            "de5.1.num_batches_tracked\n",
            "de5.3.weight\n",
            "de5.3.bias\n",
            "de5.4.conv1x1.weight\n",
            "de5.4.conv1.0.weight\n",
            "de5.4.conv1.0.bias\n",
            "de5.4.conv1.1.weight\n",
            "de5.4.conv1.1.bias\n",
            "de5.4.conv1.1.running_mean\n",
            "de5.4.conv1.1.running_var\n",
            "de5.4.conv1.1.num_batches_tracked\n",
            "de5.4.conv2.0.weight\n",
            "de5.4.conv2.1.weight\n",
            "de5.4.conv2.1.bias\n",
            "de5.4.conv2.1.running_mean\n",
            "de5.4.conv2.1.running_var\n",
            "de5.4.conv2.1.num_batches_tracked\n",
            "de5.4.conv2.3.weight\n",
            "de5.4.conv2.3.bias\n",
            "de5.4.conv2.4.weight\n",
            "de5.4.conv2.4.bias\n",
            "de5.4.conv2.4.running_mean\n",
            "de5.4.conv2.4.running_var\n",
            "de5.4.conv2.4.num_batches_tracked\n",
            "de5.4.conv3.0.weight\n",
            "de5.4.conv3.1.weight\n",
            "de5.4.conv3.1.bias\n",
            "de5.4.conv3.1.running_mean\n",
            "de5.4.conv3.1.running_var\n",
            "de5.4.conv3.1.num_batches_tracked\n",
            "de5.4.conv3.3.weight\n",
            "de5.4.conv3.3.bias\n",
            "de5.4.conv3.4.weight\n",
            "de5.4.conv3.4.bias\n",
            "de5.4.conv3.4.running_mean\n",
            "de5.4.conv3.4.running_var\n",
            "de5.4.conv3.4.num_batches_tracked\n",
            "de5.4.conv4.0.weight\n",
            "de5.4.conv4.1.weight\n",
            "de5.4.conv4.1.bias\n",
            "de5.4.conv4.1.running_mean\n",
            "de5.4.conv4.1.running_var\n",
            "de5.4.conv4.1.num_batches_tracked\n",
            "de5.4.conv4.3.weight\n",
            "de5.4.conv4.3.bias\n",
            "de5.4.conv4.4.weight\n",
            "de5.4.conv4.4.bias\n",
            "de5.4.conv4.4.running_mean\n",
            "de5.4.conv4.4.running_var\n",
            "de5.4.conv4.4.num_batches_tracked\n",
            "de6.0.weight\n",
            "de6.0.bias\n",
            "de6.1.weight\n",
            "de6.1.bias\n",
            "de6.1.running_mean\n",
            "de6.1.running_var\n",
            "de6.1.num_batches_tracked\n",
            "de7.0.weight\n",
            "de7.0.bias\n",
            "de7.1.weight\n",
            "de7.1.bias\n",
            "de7.1.running_mean\n",
            "de7.1.running_var\n",
            "de7.1.num_batches_tracked\n",
            "de8.0.weight\n",
            "de8.0.bias\n",
            "de8.1.weight\n",
            "de8.1.bias\n",
            "de8.1.running_mean\n",
            "de8.1.running_var\n",
            "de8.1.num_batches_tracked\n",
            "de9.0.weight\n",
            "de9.0.bias\n",
            "de9.1.weight\n",
            "de9.1.bias\n",
            "de9.1.running_mean\n",
            "de9.1.running_var\n",
            "de9.1.num_batches_tracked\n",
            "Optimizer State Dictionary Keys:\n",
            "state\n",
            "param_groups\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual Training Part"
      ],
      "metadata": {
        "id": "fIGkOW_cLCLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "bs = 256\n",
        "length = 1250\n",
        "epochs = 20\n",
        "folds = 5\n",
        "\n",
        "\n",
        "# Initialize the model and load pretrained weights if available\n",
        "model = Unet((bs, 1, length)).cuda()\n",
        "path = 'model/ssl.pt'  # Path to your pretrained model checkpoint\n",
        "checkpoint = torch.load(path)\n",
        "pretrained_dict = {k: v for k, v in checkpoint['model'].items() if re.search('^e|^i', k)}\n",
        "\n",
        "model_dict = model.state_dict()\n",
        "model_dict.update(pretrained_dict)\n",
        "model.load_state_dict(model_dict)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = torch.nn.SmoothL1Loss()\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 200], gamma=0.1)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "early_stopper = EarlyStopper(patience=100)  # Adjust patience according to your needs\n",
        "\n",
        "best_loss = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_loss_v = 0.0\n",
        "\n",
        "    for fold in range(1, folds + 1):\n",
        "        train_loader = DataLoader(BPdatasetv2(fold-1, train=True), batch_size=bs, shuffle=True)\n",
        "        val_loader = DataLoader(BPdatasetv2(fold-1, val=True), batch_size=bs, shuffle=False)\n",
        "\n",
        "        for idx, (inputs, output) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "            inputs = inputs.cuda()\n",
        "            output = output.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                pred = model(inputs)\n",
        "                loss = criterion(pred, output)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # VALIDATION\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for idx, (inputs_v, labels_v) in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
        "                inputs_v = inputs_v.cuda()\n",
        "                labels_v = labels_v.cuda()\n",
        "                outputs_v = model(inputs_v).cuda()\n",
        "                loss_v = criterion(outputs_v, labels_v)\n",
        "                running_loss_v += loss_v.item() * inputs_v.size(0)\n",
        "\n",
        "    avg_train_loss = running_loss / (len(train_loader.dataset) * folds)\n",
        "    avg_val_loss = running_loss_v / (len(val_loader.dataset) * folds)\n",
        "\n",
        "    path = 'final.pt'\n",
        "\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'best_dev_loss': best_loss,\n",
        "            'exp_dir': 'model'\n",
        "        }, f=path)\n",
        "    print('Loss: {:.4f}   Val_loss: {:.4f}'.format(avg_train_loss, avg_val_loss))\n",
        "\n",
        "    if early_stopper.early_stop(avg_val_loss):\n",
        "        print(\"Early stopping\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-G5RY4L-yeS",
        "outputId": "c09f3460-320a-4e31-9c79-a6cc0027cfa7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.14it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.61it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.55it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.44it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.54it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.34it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.27it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.14it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.40it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0177   Val_loss: 0.0259\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.20it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.73it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.48it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.54it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.48it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.03it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.54it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.54it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0137   Val_loss: 0.0138\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.20it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.71it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.55it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.85it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.44it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.56it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.54it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.70it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.55it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0112   Val_loss: 0.0092\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.00it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.03it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.60it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.49it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.67it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.59it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.50it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0090   Val_loss: 0.0088\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.23it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.69it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.57it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.70it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.47it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.86it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.59it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0088   Val_loss: 0.0080\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.21it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.61it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.55it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.52it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.54it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.32it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.55it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.65it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.40it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0076   Val_loss: 0.0071\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.27it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.49it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.45it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.43it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.54it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.61it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.53it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.62it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.52it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0071   Val_loss: 0.0067\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.05it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.61it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.57it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.73it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.34it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.61it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.55it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.96it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.40it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0064   Val_loss: 0.0060\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.24it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.64it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.49it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.65it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.43it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.99it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.57it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.61it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0061   Val_loss: 0.0059\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.13it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.72it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.50it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 21.83it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.53it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.73it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.56it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.72it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.54it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0058   Val_loss: 0.0055\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.01it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.16it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.55it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.51it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.41it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.72it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.47it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0056   Val_loss: 0.0053\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.27it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.81it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.56it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.57it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.35it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.53it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.29it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.52it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0056   Val_loss: 0.0071\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.26it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.65it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.45it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.44it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.52it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.21it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.53it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.56it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0059   Val_loss: 0.0058\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.20it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.38it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.48it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.33it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.53it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.59it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.64it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.52it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0054   Val_loss: 0.0054\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 10.99it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.35it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.55it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.56it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.62it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.34it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.49it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0052   Val_loss: 0.0050\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.29it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.74it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.75it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.60it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.46it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.55it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.13it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.47it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0052   Val_loss: 0.0057\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.30it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.73it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.61it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.48it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.50it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.17it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.50it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.62it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.60it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0052   Val_loss: 0.0051\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.30it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.66it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.60it/s]\n",
            "100%|██████████| 40/40 [00:02<00:00, 14.66it/s]\n",
            "100%|██████████| 157/157 [00:15<00:00,  9.94it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.53it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.45it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.39it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.45it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0053   Val_loss: 0.0050\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:13<00:00, 11.24it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.80it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.56it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.78it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.62it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.38it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.50it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.21it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.58it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0051   Val_loss: 0.0049\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:14<00:00, 11.21it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.63it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.48it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.32it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.47it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.16it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.63it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]\n",
            "100%|██████████| 157/157 [00:13<00:00, 11.46it/s]\n",
            "100%|██████████| 40/40 [00:01<00:00, 22.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0049   Val_loss: 0.0052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vu3Dhm2Egv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check final trained model"
      ],
      "metadata": {
        "id": "mHR1bcJ3LH-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the path to your checkpoint file\n",
        "path = 'final.pt'\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(path)\n",
        "\n",
        "# Extract components from the checkpoint\n",
        "epoch = checkpoint['epoch']  # Epoch number when checkpoint was saved\n",
        "model_state_dict = checkpoint['model']  # State dictionary of the model\n",
        "optimizer_state_dict = checkpoint['optimizer']  # State dictionary of the optimizer\n",
        "best_dev_loss = checkpoint['best_dev_loss']  # Best validation loss recorded\n",
        "exp_dir = checkpoint['exp_dir']  # Directory or other metadata related to the experiment\n",
        "\n",
        "# Print or inspect these components as needed\n",
        "print(f'Epoch: {epoch}')\n",
        "print(f'Best Validation Loss: {best_dev_loss}')\n",
        "print(f'Experiment Directory: {exp_dir}')\n",
        "\n",
        "# Example of inspecting the model state dictionary keys\n",
        "print('Model State Dictionary Keys:')\n",
        "for key in model_state_dict.keys():\n",
        "    print(key)\n",
        "\n",
        "# Example of inspecting optimizer state dictionary keys\n",
        "print('Optimizer State Dictionary Keys:')\n",
        "for key in optimizer_state_dict.keys():\n",
        "    print(key)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "cd917b3c-c844-4766-c57e-441a97838e7c",
        "id": "rZ4DOKCHEhBV"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18\n",
            "Best Validation Loss: 0.00494062293574214\n",
            "Experiment Directory: model\n",
            "Model State Dictionary Keys:\n",
            "inter.0.conv1.weight\n",
            "inter.0.conv1.bias\n",
            "inter.0.bn1.weight\n",
            "inter.0.bn1.bias\n",
            "inter.0.bn1.running_mean\n",
            "inter.0.bn1.running_var\n",
            "inter.0.bn1.num_batches_tracked\n",
            "inter.0.conv2.weight\n",
            "inter.0.conv2.bias\n",
            "inter.0.bn2.weight\n",
            "inter.0.bn2.bias\n",
            "inter.0.bn2.running_mean\n",
            "inter.0.bn2.running_var\n",
            "inter.0.bn2.num_batches_tracked\n",
            "inter.0.conv3.weight\n",
            "inter.0.conv3.bias\n",
            "inter.0.bn3.weight\n",
            "inter.0.bn3.bias\n",
            "inter.0.bn3.running_mean\n",
            "inter.0.bn3.running_var\n",
            "inter.0.bn3.num_batches_tracked\n",
            "inter.0.conv4.weight\n",
            "inter.0.conv4.bias\n",
            "inter.0.bn4.weight\n",
            "inter.0.bn4.bias\n",
            "inter.0.bn4.running_mean\n",
            "inter.0.bn4.running_var\n",
            "inter.0.bn4.num_batches_tracked\n",
            "inter.0.conv5.weight\n",
            "inter.0.conv5.bias\n",
            "inter.0.bn5.weight\n",
            "inter.0.bn5.bias\n",
            "inter.0.bn5.running_mean\n",
            "inter.0.bn5.running_var\n",
            "inter.0.bn5.num_batches_tracked\n",
            "en1.0.weight\n",
            "en1.0.bias\n",
            "en1.1.weight\n",
            "en1.1.bias\n",
            "en1.1.running_mean\n",
            "en1.1.running_var\n",
            "en1.1.num_batches_tracked\n",
            "en1.3.weight\n",
            "en1.3.bias\n",
            "en1.4.conv1x1.weight\n",
            "en1.4.conv1.0.weight\n",
            "en1.4.conv1.0.bias\n",
            "en1.4.conv1.1.weight\n",
            "en1.4.conv1.1.bias\n",
            "en1.4.conv1.1.running_mean\n",
            "en1.4.conv1.1.running_var\n",
            "en1.4.conv1.1.num_batches_tracked\n",
            "en1.4.conv2.0.weight\n",
            "en1.4.conv2.1.weight\n",
            "en1.4.conv2.1.bias\n",
            "en1.4.conv2.1.running_mean\n",
            "en1.4.conv2.1.running_var\n",
            "en1.4.conv2.1.num_batches_tracked\n",
            "en1.4.conv2.3.weight\n",
            "en1.4.conv2.3.bias\n",
            "en1.4.conv2.4.weight\n",
            "en1.4.conv2.4.bias\n",
            "en1.4.conv2.4.running_mean\n",
            "en1.4.conv2.4.running_var\n",
            "en1.4.conv2.4.num_batches_tracked\n",
            "en1.4.conv3.0.weight\n",
            "en1.4.conv3.1.weight\n",
            "en1.4.conv3.1.bias\n",
            "en1.4.conv3.1.running_mean\n",
            "en1.4.conv3.1.running_var\n",
            "en1.4.conv3.1.num_batches_tracked\n",
            "en1.4.conv3.3.weight\n",
            "en1.4.conv3.3.bias\n",
            "en1.4.conv3.4.weight\n",
            "en1.4.conv3.4.bias\n",
            "en1.4.conv3.4.running_mean\n",
            "en1.4.conv3.4.running_var\n",
            "en1.4.conv3.4.num_batches_tracked\n",
            "en1.4.conv4.0.weight\n",
            "en1.4.conv4.1.weight\n",
            "en1.4.conv4.1.bias\n",
            "en1.4.conv4.1.running_mean\n",
            "en1.4.conv4.1.running_var\n",
            "en1.4.conv4.1.num_batches_tracked\n",
            "en1.4.conv4.3.weight\n",
            "en1.4.conv4.3.bias\n",
            "en1.4.conv4.4.weight\n",
            "en1.4.conv4.4.bias\n",
            "en1.4.conv4.4.running_mean\n",
            "en1.4.conv4.4.running_var\n",
            "en1.4.conv4.4.num_batches_tracked\n",
            "en2.0.weight\n",
            "en2.0.bias\n",
            "en2.1.weight\n",
            "en2.1.bias\n",
            "en2.1.running_mean\n",
            "en2.1.running_var\n",
            "en2.1.num_batches_tracked\n",
            "en2.3.weight\n",
            "en2.3.bias\n",
            "en2.4.conv1x1.weight\n",
            "en2.4.conv1.0.weight\n",
            "en2.4.conv1.0.bias\n",
            "en2.4.conv1.1.weight\n",
            "en2.4.conv1.1.bias\n",
            "en2.4.conv1.1.running_mean\n",
            "en2.4.conv1.1.running_var\n",
            "en2.4.conv1.1.num_batches_tracked\n",
            "en2.4.conv2.0.weight\n",
            "en2.4.conv2.1.weight\n",
            "en2.4.conv2.1.bias\n",
            "en2.4.conv2.1.running_mean\n",
            "en2.4.conv2.1.running_var\n",
            "en2.4.conv2.1.num_batches_tracked\n",
            "en2.4.conv2.3.weight\n",
            "en2.4.conv2.3.bias\n",
            "en2.4.conv2.4.weight\n",
            "en2.4.conv2.4.bias\n",
            "en2.4.conv2.4.running_mean\n",
            "en2.4.conv2.4.running_var\n",
            "en2.4.conv2.4.num_batches_tracked\n",
            "en2.4.conv3.0.weight\n",
            "en2.4.conv3.1.weight\n",
            "en2.4.conv3.1.bias\n",
            "en2.4.conv3.1.running_mean\n",
            "en2.4.conv3.1.running_var\n",
            "en2.4.conv3.1.num_batches_tracked\n",
            "en2.4.conv3.3.weight\n",
            "en2.4.conv3.3.bias\n",
            "en2.4.conv3.4.weight\n",
            "en2.4.conv3.4.bias\n",
            "en2.4.conv3.4.running_mean\n",
            "en2.4.conv3.4.running_var\n",
            "en2.4.conv3.4.num_batches_tracked\n",
            "en2.4.conv4.0.weight\n",
            "en2.4.conv4.1.weight\n",
            "en2.4.conv4.1.bias\n",
            "en2.4.conv4.1.running_mean\n",
            "en2.4.conv4.1.running_var\n",
            "en2.4.conv4.1.num_batches_tracked\n",
            "en2.4.conv4.3.weight\n",
            "en2.4.conv4.3.bias\n",
            "en2.4.conv4.4.weight\n",
            "en2.4.conv4.4.bias\n",
            "en2.4.conv4.4.running_mean\n",
            "en2.4.conv4.4.running_var\n",
            "en2.4.conv4.4.num_batches_tracked\n",
            "en3.0.weight\n",
            "en3.0.bias\n",
            "en3.1.weight\n",
            "en3.1.bias\n",
            "en3.1.running_mean\n",
            "en3.1.running_var\n",
            "en3.1.num_batches_tracked\n",
            "en3.3.weight\n",
            "en3.3.bias\n",
            "en3.4.conv1x1.weight\n",
            "en3.4.conv1.0.weight\n",
            "en3.4.conv1.0.bias\n",
            "en3.4.conv1.1.weight\n",
            "en3.4.conv1.1.bias\n",
            "en3.4.conv1.1.running_mean\n",
            "en3.4.conv1.1.running_var\n",
            "en3.4.conv1.1.num_batches_tracked\n",
            "en3.4.conv2.0.weight\n",
            "en3.4.conv2.1.weight\n",
            "en3.4.conv2.1.bias\n",
            "en3.4.conv2.1.running_mean\n",
            "en3.4.conv2.1.running_var\n",
            "en3.4.conv2.1.num_batches_tracked\n",
            "en3.4.conv2.3.weight\n",
            "en3.4.conv2.3.bias\n",
            "en3.4.conv2.4.weight\n",
            "en3.4.conv2.4.bias\n",
            "en3.4.conv2.4.running_mean\n",
            "en3.4.conv2.4.running_var\n",
            "en3.4.conv2.4.num_batches_tracked\n",
            "en3.4.conv3.0.weight\n",
            "en3.4.conv3.1.weight\n",
            "en3.4.conv3.1.bias\n",
            "en3.4.conv3.1.running_mean\n",
            "en3.4.conv3.1.running_var\n",
            "en3.4.conv3.1.num_batches_tracked\n",
            "en3.4.conv3.3.weight\n",
            "en3.4.conv3.3.bias\n",
            "en3.4.conv3.4.weight\n",
            "en3.4.conv3.4.bias\n",
            "en3.4.conv3.4.running_mean\n",
            "en3.4.conv3.4.running_var\n",
            "en3.4.conv3.4.num_batches_tracked\n",
            "en3.4.conv4.0.weight\n",
            "en3.4.conv4.1.weight\n",
            "en3.4.conv4.1.bias\n",
            "en3.4.conv4.1.running_mean\n",
            "en3.4.conv4.1.running_var\n",
            "en3.4.conv4.1.num_batches_tracked\n",
            "en3.4.conv4.3.weight\n",
            "en3.4.conv4.3.bias\n",
            "en3.4.conv4.4.weight\n",
            "en3.4.conv4.4.bias\n",
            "en3.4.conv4.4.running_mean\n",
            "en3.4.conv4.4.running_var\n",
            "en3.4.conv4.4.num_batches_tracked\n",
            "en4.0.weight\n",
            "en4.0.bias\n",
            "en4.1.weight\n",
            "en4.1.bias\n",
            "en4.1.running_mean\n",
            "en4.1.running_var\n",
            "en4.1.num_batches_tracked\n",
            "en4.3.weight\n",
            "en4.3.bias\n",
            "en4.4.conv1x1.weight\n",
            "en4.4.conv1.0.weight\n",
            "en4.4.conv1.0.bias\n",
            "en4.4.conv1.1.weight\n",
            "en4.4.conv1.1.bias\n",
            "en4.4.conv1.1.running_mean\n",
            "en4.4.conv1.1.running_var\n",
            "en4.4.conv1.1.num_batches_tracked\n",
            "en4.4.conv2.0.weight\n",
            "en4.4.conv2.1.weight\n",
            "en4.4.conv2.1.bias\n",
            "en4.4.conv2.1.running_mean\n",
            "en4.4.conv2.1.running_var\n",
            "en4.4.conv2.1.num_batches_tracked\n",
            "en4.4.conv2.3.weight\n",
            "en4.4.conv2.3.bias\n",
            "en4.4.conv2.4.weight\n",
            "en4.4.conv2.4.bias\n",
            "en4.4.conv2.4.running_mean\n",
            "en4.4.conv2.4.running_var\n",
            "en4.4.conv2.4.num_batches_tracked\n",
            "en4.4.conv3.0.weight\n",
            "en4.4.conv3.1.weight\n",
            "en4.4.conv3.1.bias\n",
            "en4.4.conv3.1.running_mean\n",
            "en4.4.conv3.1.running_var\n",
            "en4.4.conv3.1.num_batches_tracked\n",
            "en4.4.conv3.3.weight\n",
            "en4.4.conv3.3.bias\n",
            "en4.4.conv3.4.weight\n",
            "en4.4.conv3.4.bias\n",
            "en4.4.conv3.4.running_mean\n",
            "en4.4.conv3.4.running_var\n",
            "en4.4.conv3.4.num_batches_tracked\n",
            "en4.4.conv4.0.weight\n",
            "en4.4.conv4.1.weight\n",
            "en4.4.conv4.1.bias\n",
            "en4.4.conv4.1.running_mean\n",
            "en4.4.conv4.1.running_var\n",
            "en4.4.conv4.1.num_batches_tracked\n",
            "en4.4.conv4.3.weight\n",
            "en4.4.conv4.3.bias\n",
            "en4.4.conv4.4.weight\n",
            "en4.4.conv4.4.bias\n",
            "en4.4.conv4.4.running_mean\n",
            "en4.4.conv4.4.running_var\n",
            "en4.4.conv4.4.num_batches_tracked\n",
            "en5.0.weight\n",
            "en5.0.bias\n",
            "en5.1.weight\n",
            "en5.1.bias\n",
            "en5.1.running_mean\n",
            "en5.1.running_var\n",
            "en5.1.num_batches_tracked\n",
            "en5.3.conv1x1.weight\n",
            "en5.3.conv1.0.weight\n",
            "en5.3.conv1.0.bias\n",
            "en5.3.conv1.1.weight\n",
            "en5.3.conv1.1.bias\n",
            "en5.3.conv1.1.running_mean\n",
            "en5.3.conv1.1.running_var\n",
            "en5.3.conv1.1.num_batches_tracked\n",
            "en5.3.conv2.0.weight\n",
            "en5.3.conv2.1.weight\n",
            "en5.3.conv2.1.bias\n",
            "en5.3.conv2.1.running_mean\n",
            "en5.3.conv2.1.running_var\n",
            "en5.3.conv2.1.num_batches_tracked\n",
            "en5.3.conv2.3.weight\n",
            "en5.3.conv2.3.bias\n",
            "en5.3.conv2.4.weight\n",
            "en5.3.conv2.4.bias\n",
            "en5.3.conv2.4.running_mean\n",
            "en5.3.conv2.4.running_var\n",
            "en5.3.conv2.4.num_batches_tracked\n",
            "en5.3.conv3.0.weight\n",
            "en5.3.conv3.1.weight\n",
            "en5.3.conv3.1.bias\n",
            "en5.3.conv3.1.running_mean\n",
            "en5.3.conv3.1.running_var\n",
            "en5.3.conv3.1.num_batches_tracked\n",
            "en5.3.conv3.3.weight\n",
            "en5.3.conv3.3.bias\n",
            "en5.3.conv3.4.weight\n",
            "en5.3.conv3.4.bias\n",
            "en5.3.conv3.4.running_mean\n",
            "en5.3.conv3.4.running_var\n",
            "en5.3.conv3.4.num_batches_tracked\n",
            "en5.3.conv4.0.weight\n",
            "en5.3.conv4.1.weight\n",
            "en5.3.conv4.1.bias\n",
            "en5.3.conv4.1.running_mean\n",
            "en5.3.conv4.1.running_var\n",
            "en5.3.conv4.1.num_batches_tracked\n",
            "en5.3.conv4.3.weight\n",
            "en5.3.conv4.3.bias\n",
            "en5.3.conv4.4.weight\n",
            "en5.3.conv4.4.bias\n",
            "en5.3.conv4.4.running_mean\n",
            "en5.3.conv4.4.running_var\n",
            "en5.3.conv4.4.num_batches_tracked\n",
            "de1.0.weight\n",
            "de1.0.bias\n",
            "de1.1.weight\n",
            "de1.1.bias\n",
            "de1.1.running_mean\n",
            "de1.1.running_var\n",
            "de1.1.num_batches_tracked\n",
            "de1.3.conv1x1.weight\n",
            "de1.3.conv1.0.weight\n",
            "de1.3.conv1.0.bias\n",
            "de1.3.conv1.1.weight\n",
            "de1.3.conv1.1.bias\n",
            "de1.3.conv1.1.running_mean\n",
            "de1.3.conv1.1.running_var\n",
            "de1.3.conv1.1.num_batches_tracked\n",
            "de1.3.conv2.0.weight\n",
            "de1.3.conv2.1.weight\n",
            "de1.3.conv2.1.bias\n",
            "de1.3.conv2.1.running_mean\n",
            "de1.3.conv2.1.running_var\n",
            "de1.3.conv2.1.num_batches_tracked\n",
            "de1.3.conv2.3.weight\n",
            "de1.3.conv2.3.bias\n",
            "de1.3.conv2.4.weight\n",
            "de1.3.conv2.4.bias\n",
            "de1.3.conv2.4.running_mean\n",
            "de1.3.conv2.4.running_var\n",
            "de1.3.conv2.4.num_batches_tracked\n",
            "de1.3.conv3.0.weight\n",
            "de1.3.conv3.1.weight\n",
            "de1.3.conv3.1.bias\n",
            "de1.3.conv3.1.running_mean\n",
            "de1.3.conv3.1.running_var\n",
            "de1.3.conv3.1.num_batches_tracked\n",
            "de1.3.conv3.3.weight\n",
            "de1.3.conv3.3.bias\n",
            "de1.3.conv3.4.weight\n",
            "de1.3.conv3.4.bias\n",
            "de1.3.conv3.4.running_mean\n",
            "de1.3.conv3.4.running_var\n",
            "de1.3.conv3.4.num_batches_tracked\n",
            "de1.3.conv4.0.weight\n",
            "de1.3.conv4.1.weight\n",
            "de1.3.conv4.1.bias\n",
            "de1.3.conv4.1.running_mean\n",
            "de1.3.conv4.1.running_var\n",
            "de1.3.conv4.1.num_batches_tracked\n",
            "de1.3.conv4.3.weight\n",
            "de1.3.conv4.3.bias\n",
            "de1.3.conv4.4.weight\n",
            "de1.3.conv4.4.bias\n",
            "de1.3.conv4.4.running_mean\n",
            "de1.3.conv4.4.running_var\n",
            "de1.3.conv4.4.num_batches_tracked\n",
            "de2.0.weight\n",
            "de2.0.bias\n",
            "de2.1.weight\n",
            "de2.1.bias\n",
            "de2.1.running_mean\n",
            "de2.1.running_var\n",
            "de2.1.num_batches_tracked\n",
            "de2.3.weight\n",
            "de2.3.bias\n",
            "de2.4.conv1x1.weight\n",
            "de2.4.conv1.0.weight\n",
            "de2.4.conv1.0.bias\n",
            "de2.4.conv1.1.weight\n",
            "de2.4.conv1.1.bias\n",
            "de2.4.conv1.1.running_mean\n",
            "de2.4.conv1.1.running_var\n",
            "de2.4.conv1.1.num_batches_tracked\n",
            "de2.4.conv2.0.weight\n",
            "de2.4.conv2.1.weight\n",
            "de2.4.conv2.1.bias\n",
            "de2.4.conv2.1.running_mean\n",
            "de2.4.conv2.1.running_var\n",
            "de2.4.conv2.1.num_batches_tracked\n",
            "de2.4.conv2.3.weight\n",
            "de2.4.conv2.3.bias\n",
            "de2.4.conv2.4.weight\n",
            "de2.4.conv2.4.bias\n",
            "de2.4.conv2.4.running_mean\n",
            "de2.4.conv2.4.running_var\n",
            "de2.4.conv2.4.num_batches_tracked\n",
            "de2.4.conv3.0.weight\n",
            "de2.4.conv3.1.weight\n",
            "de2.4.conv3.1.bias\n",
            "de2.4.conv3.1.running_mean\n",
            "de2.4.conv3.1.running_var\n",
            "de2.4.conv3.1.num_batches_tracked\n",
            "de2.4.conv3.3.weight\n",
            "de2.4.conv3.3.bias\n",
            "de2.4.conv3.4.weight\n",
            "de2.4.conv3.4.bias\n",
            "de2.4.conv3.4.running_mean\n",
            "de2.4.conv3.4.running_var\n",
            "de2.4.conv3.4.num_batches_tracked\n",
            "de2.4.conv4.0.weight\n",
            "de2.4.conv4.1.weight\n",
            "de2.4.conv4.1.bias\n",
            "de2.4.conv4.1.running_mean\n",
            "de2.4.conv4.1.running_var\n",
            "de2.4.conv4.1.num_batches_tracked\n",
            "de2.4.conv4.3.weight\n",
            "de2.4.conv4.3.bias\n",
            "de2.4.conv4.4.weight\n",
            "de2.4.conv4.4.bias\n",
            "de2.4.conv4.4.running_mean\n",
            "de2.4.conv4.4.running_var\n",
            "de2.4.conv4.4.num_batches_tracked\n",
            "de3.0.weight\n",
            "de3.0.bias\n",
            "de3.1.weight\n",
            "de3.1.bias\n",
            "de3.1.running_mean\n",
            "de3.1.running_var\n",
            "de3.1.num_batches_tracked\n",
            "de3.3.weight\n",
            "de3.3.bias\n",
            "de3.4.conv1x1.weight\n",
            "de3.4.conv1.0.weight\n",
            "de3.4.conv1.0.bias\n",
            "de3.4.conv1.1.weight\n",
            "de3.4.conv1.1.bias\n",
            "de3.4.conv1.1.running_mean\n",
            "de3.4.conv1.1.running_var\n",
            "de3.4.conv1.1.num_batches_tracked\n",
            "de3.4.conv2.0.weight\n",
            "de3.4.conv2.1.weight\n",
            "de3.4.conv2.1.bias\n",
            "de3.4.conv2.1.running_mean\n",
            "de3.4.conv2.1.running_var\n",
            "de3.4.conv2.1.num_batches_tracked\n",
            "de3.4.conv2.3.weight\n",
            "de3.4.conv2.3.bias\n",
            "de3.4.conv2.4.weight\n",
            "de3.4.conv2.4.bias\n",
            "de3.4.conv2.4.running_mean\n",
            "de3.4.conv2.4.running_var\n",
            "de3.4.conv2.4.num_batches_tracked\n",
            "de3.4.conv3.0.weight\n",
            "de3.4.conv3.1.weight\n",
            "de3.4.conv3.1.bias\n",
            "de3.4.conv3.1.running_mean\n",
            "de3.4.conv3.1.running_var\n",
            "de3.4.conv3.1.num_batches_tracked\n",
            "de3.4.conv3.3.weight\n",
            "de3.4.conv3.3.bias\n",
            "de3.4.conv3.4.weight\n",
            "de3.4.conv3.4.bias\n",
            "de3.4.conv3.4.running_mean\n",
            "de3.4.conv3.4.running_var\n",
            "de3.4.conv3.4.num_batches_tracked\n",
            "de3.4.conv4.0.weight\n",
            "de3.4.conv4.1.weight\n",
            "de3.4.conv4.1.bias\n",
            "de3.4.conv4.1.running_mean\n",
            "de3.4.conv4.1.running_var\n",
            "de3.4.conv4.1.num_batches_tracked\n",
            "de3.4.conv4.3.weight\n",
            "de3.4.conv4.3.bias\n",
            "de3.4.conv4.4.weight\n",
            "de3.4.conv4.4.bias\n",
            "de3.4.conv4.4.running_mean\n",
            "de3.4.conv4.4.running_var\n",
            "de3.4.conv4.4.num_batches_tracked\n",
            "de4.0.weight\n",
            "de4.0.bias\n",
            "de4.1.weight\n",
            "de4.1.bias\n",
            "de4.1.running_mean\n",
            "de4.1.running_var\n",
            "de4.1.num_batches_tracked\n",
            "de4.3.weight\n",
            "de4.3.bias\n",
            "de4.4.conv1x1.weight\n",
            "de4.4.conv1.0.weight\n",
            "de4.4.conv1.0.bias\n",
            "de4.4.conv1.1.weight\n",
            "de4.4.conv1.1.bias\n",
            "de4.4.conv1.1.running_mean\n",
            "de4.4.conv1.1.running_var\n",
            "de4.4.conv1.1.num_batches_tracked\n",
            "de4.4.conv2.0.weight\n",
            "de4.4.conv2.1.weight\n",
            "de4.4.conv2.1.bias\n",
            "de4.4.conv2.1.running_mean\n",
            "de4.4.conv2.1.running_var\n",
            "de4.4.conv2.1.num_batches_tracked\n",
            "de4.4.conv2.3.weight\n",
            "de4.4.conv2.3.bias\n",
            "de4.4.conv2.4.weight\n",
            "de4.4.conv2.4.bias\n",
            "de4.4.conv2.4.running_mean\n",
            "de4.4.conv2.4.running_var\n",
            "de4.4.conv2.4.num_batches_tracked\n",
            "de4.4.conv3.0.weight\n",
            "de4.4.conv3.1.weight\n",
            "de4.4.conv3.1.bias\n",
            "de4.4.conv3.1.running_mean\n",
            "de4.4.conv3.1.running_var\n",
            "de4.4.conv3.1.num_batches_tracked\n",
            "de4.4.conv3.3.weight\n",
            "de4.4.conv3.3.bias\n",
            "de4.4.conv3.4.weight\n",
            "de4.4.conv3.4.bias\n",
            "de4.4.conv3.4.running_mean\n",
            "de4.4.conv3.4.running_var\n",
            "de4.4.conv3.4.num_batches_tracked\n",
            "de4.4.conv4.0.weight\n",
            "de4.4.conv4.1.weight\n",
            "de4.4.conv4.1.bias\n",
            "de4.4.conv4.1.running_mean\n",
            "de4.4.conv4.1.running_var\n",
            "de4.4.conv4.1.num_batches_tracked\n",
            "de4.4.conv4.3.weight\n",
            "de4.4.conv4.3.bias\n",
            "de4.4.conv4.4.weight\n",
            "de4.4.conv4.4.bias\n",
            "de4.4.conv4.4.running_mean\n",
            "de4.4.conv4.4.running_var\n",
            "de4.4.conv4.4.num_batches_tracked\n",
            "de5.0.weight\n",
            "de5.0.bias\n",
            "de5.1.weight\n",
            "de5.1.bias\n",
            "de5.1.running_mean\n",
            "de5.1.running_var\n",
            "de5.1.num_batches_tracked\n",
            "de5.3.weight\n",
            "de5.3.bias\n",
            "de5.4.conv1x1.weight\n",
            "de5.4.conv1.0.weight\n",
            "de5.4.conv1.0.bias\n",
            "de5.4.conv1.1.weight\n",
            "de5.4.conv1.1.bias\n",
            "de5.4.conv1.1.running_mean\n",
            "de5.4.conv1.1.running_var\n",
            "de5.4.conv1.1.num_batches_tracked\n",
            "de5.4.conv2.0.weight\n",
            "de5.4.conv2.1.weight\n",
            "de5.4.conv2.1.bias\n",
            "de5.4.conv2.1.running_mean\n",
            "de5.4.conv2.1.running_var\n",
            "de5.4.conv2.1.num_batches_tracked\n",
            "de5.4.conv2.3.weight\n",
            "de5.4.conv2.3.bias\n",
            "de5.4.conv2.4.weight\n",
            "de5.4.conv2.4.bias\n",
            "de5.4.conv2.4.running_mean\n",
            "de5.4.conv2.4.running_var\n",
            "de5.4.conv2.4.num_batches_tracked\n",
            "de5.4.conv3.0.weight\n",
            "de5.4.conv3.1.weight\n",
            "de5.4.conv3.1.bias\n",
            "de5.4.conv3.1.running_mean\n",
            "de5.4.conv3.1.running_var\n",
            "de5.4.conv3.1.num_batches_tracked\n",
            "de5.4.conv3.3.weight\n",
            "de5.4.conv3.3.bias\n",
            "de5.4.conv3.4.weight\n",
            "de5.4.conv3.4.bias\n",
            "de5.4.conv3.4.running_mean\n",
            "de5.4.conv3.4.running_var\n",
            "de5.4.conv3.4.num_batches_tracked\n",
            "de5.4.conv4.0.weight\n",
            "de5.4.conv4.1.weight\n",
            "de5.4.conv4.1.bias\n",
            "de5.4.conv4.1.running_mean\n",
            "de5.4.conv4.1.running_var\n",
            "de5.4.conv4.1.num_batches_tracked\n",
            "de5.4.conv4.3.weight\n",
            "de5.4.conv4.3.bias\n",
            "de5.4.conv4.4.weight\n",
            "de5.4.conv4.4.bias\n",
            "de5.4.conv4.4.running_mean\n",
            "de5.4.conv4.4.running_var\n",
            "de5.4.conv4.4.num_batches_tracked\n",
            "de6.0.weight\n",
            "de6.0.bias\n",
            "de6.1.weight\n",
            "de6.1.bias\n",
            "de6.1.running_mean\n",
            "de6.1.running_var\n",
            "de6.1.num_batches_tracked\n",
            "de7.0.weight\n",
            "de7.0.bias\n",
            "de7.1.weight\n",
            "de7.1.bias\n",
            "de7.1.running_mean\n",
            "de7.1.running_var\n",
            "de7.1.num_batches_tracked\n",
            "de8.0.weight\n",
            "de8.0.bias\n",
            "de8.1.weight\n",
            "de8.1.bias\n",
            "de8.1.running_mean\n",
            "de8.1.running_var\n",
            "de8.1.num_batches_tracked\n",
            "de9.0.weight\n",
            "de9.0.bias\n",
            "de9.1.weight\n",
            "de9.1.bias\n",
            "de9.1.running_mean\n",
            "de9.1.running_var\n",
            "de9.1.num_batches_tracked\n",
            "Optimizer State Dictionary Keys:\n",
            "state\n",
            "param_groups\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test trained model on remaining (independent) testing set"
      ],
      "metadata": {
        "id": "53n-oUorLLsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import re\n",
        "\n",
        "bs = 256\n",
        "\n",
        "model = Unet((256,1,1250)).cuda()\n",
        "path = 'final.pt'\n",
        "checkpoint = torch.load(path)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "pick_path = 'output.p'\n",
        "\n",
        "test = torch.utils.data.DataLoader(BPdatasetv2(0, train = False, val = False,  test = True), batch_size=bs)\n",
        "\n",
        "temp1 = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx,(inputs,labels) in tqdm(enumerate(test),total=len(test),  disable=True):\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs_v= model(inputs).cuda()\n",
        "\n",
        "        temp1.extend(outputs_v)\n",
        "\n",
        "temp1 = torch.stack(temp1)\n",
        "with open(pick_path,'wb') as f:\n",
        "    pickle.dump(temp1.cpu().detach().numpy(), f)"
      ],
      "metadata": {
        "id": "zjddg6BaFZ6_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the final predictions"
      ],
      "metadata": {
        "id": "OZenJgbELTem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Define the path to your output file\n",
        "pick_path = 'output.p'\n",
        "\n",
        "# Load the pickle file\n",
        "with open(pick_path, 'rb') as f:\n",
        "    output_data = pickle.load(f)\n",
        "\n",
        "# Check the type of the loaded data\n",
        "print(f'Type of output data: {type(output_data)}')\n",
        "\n",
        "# If it's a numpy array, inspect its shape and some sample data\n",
        "if isinstance(output_data, np.ndarray):\n",
        "    print(f'Shape of output data: {output_data.shape}')\n",
        "    print('Sample data:')\n",
        "    print(output_data[:5])  # Print the first 5 entries\n",
        "\n",
        "# If it's a list of tensors, convert to a numpy array and inspect\n",
        "elif isinstance(output_data, list) and isinstance(output_data[0], torch.Tensor):\n",
        "    output_data = torch.stack(output_data).cpu().numpy()\n",
        "    print(f'Shape of output data: {output_data.shape}')\n",
        "    print('Sample data:')\n",
        "    print(output_data[:5])  # Print the first 5 entries\n",
        "\n",
        "# If it's another data type, provide appropriate inspection\n",
        "else:\n",
        "    print(f'Unexpected data type: {type(output_data)}')\n",
        "    print('Sample data:')\n",
        "    print(output_data[:5])  # Print the first 5 entries\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21bUKeCDFgC0",
        "outputId": "e34bea24-3414-4e9d-b503-49298a993b96"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of output data: <class 'numpy.ndarray'>\n",
            "Shape of output data: (10000, 1, 1250)\n",
            "Sample data:\n",
            "[[[0.46805903 0.50473255 0.5322795  ... 0.31806147 0.35078776 0.33633965]]\n",
            "\n",
            " [[0.19793008 0.18881768 0.20094647 ... 0.28478456 0.26610154 0.25896198]]\n",
            "\n",
            " [[0.16194268 0.15142511 0.14950547 ... 0.37587768 0.35324025 0.34329766]]\n",
            "\n",
            " [[0.5091641  0.50857586 0.4942793  ... 0.29647446 0.31255406 0.30774057]]\n",
            "\n",
            " [[0.18779568 0.20288466 0.20725879 ... 0.52020985 0.5465441  0.53120536]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Define the path to the output file\n",
        "pick_path = 'output.p'\n",
        "\n",
        "# Load the predictions from the pickle file\n",
        "with open(pick_path, 'rb') as f:\n",
        "    Y_pred = pickle.load(f)\n",
        "\n",
        "# Check the type and shape of the loaded predictions\n",
        "print(f'Type of Y_pred: {type(Y_pred)}')\n",
        "print(f'Shape of Y_pred: {Y_pred.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9EZN_3jGiFn",
        "outputId": "5eb42a00-8f34-4ef3-f6bf-5d395e07737a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of Y_pred: <class 'numpy.ndarray'>\n",
            "Shape of Y_pred: (10000, 1, 1250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluations standarts"
      ],
      "metadata": {
        "id": "yTisfPwyLZA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "def evaluate_BHS_Standard(filename):\n",
        "    \"\"\"\n",
        "        Evaluates PPG2ABP based on\n",
        "        BHS Standard Metric\n",
        "    \"\"\"\n",
        "\n",
        "    def BHS_metric(err):\n",
        "        \"\"\"\n",
        "        Computes the BHS Standard metric\n",
        "\n",
        "        Arguments:\n",
        "            err {array} -- array of absolute error\n",
        "\n",
        "        Returns:\n",
        "            tuple -- tuple of percentage of samples with <=5 mmHg, <=10 mmHg and <=15 mmHg error\n",
        "        \"\"\"\n",
        "\n",
        "        leq5 = 0\n",
        "        leq10 = 0\n",
        "        leq15 = 0\n",
        "\n",
        "        for i in range(len(err)):\n",
        "            if abs(err[i]) <= 5:\n",
        "                leq5 += 1\n",
        "                leq10 += 1\n",
        "                leq15 += 1\n",
        "            elif abs(err[i]) <= 10:\n",
        "                leq10 += 1\n",
        "                leq15 += 1\n",
        "            elif abs(err[i]) <= 15:\n",
        "                leq15 += 1\n",
        "\n",
        "        return (leq5 * 100.0 / len(err), leq10 * 100.0 / len(err), leq15 * 100.0 / len(err))\n",
        "\n",
        "    def calcError(Ytrue, Ypred, max_abp, min_abp):\n",
        "        \"\"\"\n",
        "        Calculates the absolute error of sbp, dbp, map etc.\n",
        "\n",
        "        Arguments:\n",
        "            Ytrue {array} -- ground truth\n",
        "            Ypred {array} -- predicted\n",
        "            max_abp {float} -- max value of abp signal\n",
        "            min_abp {float} -- min value of abp signal\n",
        "\n",
        "        Returns:\n",
        "            tuple -- tuple of abs. errors of sbp, dbp and map calculation\n",
        "        \"\"\"\n",
        "\n",
        "        sbps = []\n",
        "        dbps = []\n",
        "        maps = []\n",
        "\n",
        "        for i in range(len(Ytrue)):\n",
        "            y_t = Ytrue[i].ravel()\n",
        "            y_p = Ypred[i].ravel()\n",
        "\n",
        "            y_t = y_t * (max_abp - min_abp)\n",
        "            y_p = y_p * (max_abp - min_abp)\n",
        "\n",
        "            dbps.append(abs(min(y_t) - min(y_p)))\n",
        "            sbps.append(abs(max(y_t) - max(y_p)))\n",
        "            maps.append(abs(np.mean(y_t) - np.mean(y_p)))\n",
        "\n",
        "        return (sbps, dbps, maps)\n",
        "\n",
        "    dt = pickle.load(open(os.path.join('data', 'test.p'), 'rb'))  # loading test data\n",
        "    Y_test = dt['Y_test']\n",
        "\n",
        "    dt = pickle.load(open('meta.p', 'rb'))  # loading meta data\n",
        "    max_abp = dt['max_abp']\n",
        "    min_abp = dt['min_abp']\n",
        "\n",
        "    Y_pred = pickle.load(open(filename, 'rb'))  # loading prediction\n",
        "\n",
        "    (sbps, dbps, maps) = calcError(Y_test, Y_pred, max_abp, min_abp)  # compute errors\n",
        "\n",
        "    sbp_percent = BHS_metric(sbps)  # compute BHS metric for sbp\n",
        "    dbp_percent = BHS_metric(dbps)  # compute BHS metric for dbp\n",
        "    map_percent = BHS_metric(maps)  # compute BHS metric for map\n",
        "\n",
        "    print('----------------------------')\n",
        "    print('|        BHS-Metric        |')\n",
        "    print('----------------------------')\n",
        "\n",
        "    print('----------------------------------------')\n",
        "    print('|     | <= 5mmHg | <=10mmHg | <=15mmHg |')\n",
        "    print('----------------------------------------')\n",
        "    print('| DBP |  {} %  |  {} %  |  {} %  |'.format(round(dbp_percent[0], 2), round(dbp_percent[1], 2), round(dbp_percent[2], 2)))\n",
        "    print('| MAP |  {} %  |  {} %  |  {} %  |'.format(round(map_percent[0], 2), round(map_percent[1], 2), round(map_percent[2], 2)))\n",
        "    print('| SBP |  {} %  |  {} %  |  {} %  |'.format(round(sbp_percent[0], 2), round(sbp_percent[1], 2), round(sbp_percent[2], 2)))\n",
        "    print('----------------------------------------')\n",
        "\n",
        "\n",
        "def evaluate_AAMI_Standard(filename):\n",
        "    \"\"\"\n",
        "        Evaluate PPG2ABP using AAMI Standard metric\n",
        "    \"\"\"\n",
        "\n",
        "    def calcErrorAAMI(Ypred, Ytrue, max_abp, min_abp):\n",
        "        \"\"\"\n",
        "        Calculates error of sbp, dbp, map for AAMI standard computation\n",
        "\n",
        "        Arguments:\n",
        "            Ytrue {array} -- ground truth\n",
        "            Ypred {array} -- predicted\n",
        "            max_abp {float} -- max value of abp signal\n",
        "            min_abp {float} -- min value of abp signal\n",
        "\n",
        "        Returns:\n",
        "            tuple -- tuple of errors of sbp, dbp and map calculation\n",
        "        \"\"\"\n",
        "\n",
        "        sbps = []\n",
        "        dbps = []\n",
        "        maps = []\n",
        "\n",
        "        for i in range(len(Ytrue)):\n",
        "            y_t = Ytrue[i].ravel()\n",
        "            y_p = Ypred[i].ravel()\n",
        "\n",
        "            y_t = y_t * (max_abp - min_abp)\n",
        "            y_p = y_p * (max_abp - min_abp)\n",
        "\n",
        "            dbps.append(min(y_p) - min(y_t))\n",
        "            sbps.append(max(y_p) - max(y_t))\n",
        "            maps.append(np.mean(y_p) - np.mean(y_t))\n",
        "\n",
        "        return (sbps, dbps, maps)\n",
        "\n",
        "    dt = pickle.load(open(os.path.join('data', 'test.p'), 'rb'))  # loading test data\n",
        "    Y_test = dt['Y_test']\n",
        "\n",
        "    dt = pickle.load(open(os.path.join('data', 'meta.p'), 'rb'))  # loading metadata\n",
        "    max_abp = dt['max_abp']\n",
        "    min_abp = dt['min_abp']\n",
        "\n",
        "    Y_pred = pickle.load(open(filename, 'rb'))  # loading prediction\n",
        "\n",
        "    (sbps, dbps, maps) = calcErrorAAMI(Y_test, Y_pred, max_abp, min_abp)  # compute error\n",
        "\n",
        "    print('---------------------')\n",
        "    print('|   AAMI Standard   |')\n",
        "    print('---------------------')\n",
        "\n",
        "    print('-----------------------')\n",
        "    print('|     |  ME   |  STD  |')\n",
        "    print('-----------------------')\n",
        "    print('| DBP | {} | {} |'.format(round(np.mean(dbps), 3), round(np.std(dbps), 3)))\n",
        "    print('| MAP | {} | {} |'.format(round(np.mean(maps), 3), round(np.std(maps), 3)))\n",
        "    print('| SBP | {} | {} |'.format(round(np.mean(sbps), 3), round(np.std(sbps), 3)))\n",
        "    print('-----------------------')\n",
        "\n",
        "\n",
        "def evaluate_metrics(filename):\n",
        "    def calcError(Ytrue, Ypred, max_abp, min_abp):\n",
        "        sbp_t = []\n",
        "        sbp_p = []\n",
        "        dbp_t = []\n",
        "        dbp_p = []\n",
        "        map_t = []\n",
        "        map_p = []\n",
        "\n",
        "        for i in range(len(Ytrue)):\n",
        "            y_t = Ytrue[i].ravel()\n",
        "            y_p = Ypred[i].ravel()\n",
        "\n",
        "            y_t = y_t * (max_abp - min_abp)\n",
        "            y_p = y_p * (max_abp - min_abp)\n",
        "\n",
        "            sbp_p.append(abs(max(y_p)))\n",
        "            dbp_p.append(abs(min(y_p)))\n",
        "            map_p.append(abs(np.mean(y_p)))\n",
        "            sbp_t.append(abs(max(y_t)))\n",
        "            dbp_t.append(abs(min(y_t)))\n",
        "            map_t.append(abs(np.mean(y_t)))\n",
        "\n",
        "        print(\"SBP\")\n",
        "        print(\"Mean Absolute Error : \", round(mean_absolute_error(sbp_t, sbp_p), 3))\n",
        "        print(\"Root Mean Squared Error : \", round(mean_squared_error(sbp_t, sbp_p, squared=False), 3))\n",
        "        print(\"R2 : \", r2_score(sbp_t, sbp_p))\n",
        "\n",
        "        print(\"\")\n",
        "\n",
        "        print(\"DBP\")\n",
        "        print(\"Mean Absolute Error : \", round(mean_absolute_error(dbp_t, dbp_p), 3))\n",
        "        print(\"Root Mean Squared Error : \", round(mean_squared_error(dbp_t, dbp_p, squared=False), 3))\n",
        "        print(\"R2 : \", r2_score(dbp_t, dbp_p))\n",
        "\n",
        "        print(\"\")\n",
        "\n",
        "        print(\"MAP\")\n",
        "        print(\"Mean Absolute Error : \", mean_absolute_error(map_t, map_p))\n",
        "        print(\"Root Mean Squared Error : \", round(mean_squared_error(map_t, map_p, squared=False), 2))\n",
        "        print(\"R2 : \", r2_score(map_t, map_p))\n",
        "\n",
        "        print(\"------------------------------------------------------------------------\")\n",
        "\n",
        "    dt = pickle.load(open(os.path.join('data', 'test.p'), 'rb'))  # loading test data\n",
        "    Y_test = dt['Y_test']\n",
        "\n",
        "    dt = pickle.load(open('meta.p', 'rb'))  # loading meta data\n",
        "    max_abp = dt['max_abp']\n",
        "    min_abp = dt['min_abp']\n",
        "\n",
        "    Y_pred = pickle.load(open(filename, 'rb'))  # loading prediction\n",
        "    calcError(Y_test, Y_pred, max_abp, min_abp)\n",
        "\n",
        "\n",
        "evaluate_BHS_Standard('output.p')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHQYVVKeJUqr",
        "outputId": "f9361658-7568-42f2-93c6-afd5fb541066"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------\n",
            "|        BHS-Metric        |\n",
            "----------------------------\n",
            "----------------------------------------\n",
            "|     | <= 5mmHg | <=10mmHg | <=15mmHg |\n",
            "----------------------------------------\n",
            "| DBP |  49.05 %  |  82.52 %  |  95.32 %  |\n",
            "| MAP |  48.29 %  |  78.28 %  |  92.55 %  |\n",
            "| SBP |  28.14 %  |  53.55 %  |  71.57 %  |\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_AAMI_Standard('output.p')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBfa9UFkJbT3",
        "outputId": "6d8e8d97-6a7f-4d59-82f0-943348abef1a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "|   AAMI Standard   |\n",
            "---------------------\n",
            "-----------------------\n",
            "|     |  ME   |  STD  |\n",
            "-----------------------\n",
            "| DBP | -1.312 | 7.827 |\n",
            "| MAP | -0.193 | 8.636 |\n",
            "| SBP | -0.029 | 14.945 |\n",
            "-----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_metrics('output.p')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjT-5LNNJebV",
        "outputId": "fec508c2-67a8-4688-e034-518c80eeea98"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SBP\n",
            "Mean Absolute Error :  11.584\n",
            "Root Mean Squared Error :  14.945\n",
            "R2 :  0.30646496557721925\n",
            "\n",
            "DBP\n",
            "Mean Absolute Error :  6.113\n",
            "Root Mean Squared Error :  7.935\n",
            "R2 :  0.21471609588969098\n",
            "\n",
            "MAP\n",
            "Mean Absolute Error :  6.554343162021774\n",
            "Root Mean Squared Error :  8.64\n",
            "R2 :  0.3982593020463836\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}