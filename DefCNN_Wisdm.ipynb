{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1-utoyt-vzHjOhcvcQkshlLABStvoe92J",
      "authorship_tag": "ABX9TyN3OnzIAkHamqGIxth0LyB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arushi-lu/deep_learning/blob/main/DefCNN_Wisdm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KH8AQBHcLQO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and read the raw data file\n",
        "file = open('WISDM_ar_v1.1_raw.txt')\n",
        "lines = file.readlines()\n",
        "\n",
        "processedList = []\n",
        "\n",
        "# Process each line in the raw data file\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        # Split each line into components\n",
        "        line = line.split(',')\n",
        "        # Extract and clean the z-axis value\n",
        "        last = line[5].split(';')[0].strip()\n",
        "        if last == '':\n",
        "            break\n",
        "        # Append the cleaned data to the list\n",
        "        temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "        processedList.append(temp)\n",
        "    except:\n",
        "        print('Error at line number: ', i)\n",
        "\n",
        "# Define column names and create a DataFrame\n",
        "columns = ['user', 'activity', 'time', 'x', 'y', 'z']\n",
        "data = pd.DataFrame(data=processedList, columns=columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ2RNtF2cVS6",
        "outputId": "2e21b9a3-281b-4617-f41c-ebff9a4fafd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error at line number:  281873\n",
            "Error at line number:  281874\n",
            "Error at line number:  281875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert x, y, z columns to float\n",
        "data['x'] = data['x'].astype('float')\n",
        "data['y'] = data['y'].astype('float')\n",
        "data['z'] = data['z'].astype('float')\n",
        "\n",
        "# Sampling frequency (not used here but could be used later)\n",
        "Fs = 20\n",
        "\n",
        "# Get the list of activities\n",
        "activities = data['activity'].value_counts().index\n",
        "print(activities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UZiXJNfcXVq",
        "outputId": "da648993-0a22-4439-c267-6082418637da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Walking', 'Jogging', 'Upstairs', 'Downstairs', 'Sitting', 'Standing'], dtype='object', name='activity')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop user and time columns for further processing\n",
        "df = data.drop(['user', 'time'], axis=1).copy()\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "944wE4Z3cdX4",
        "outputId": "dcb6e290-205f-4475-ecf5-aeedefcbcf49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  activity         x          y         z\n",
            "0  Jogging -0.694638  12.680544  0.503953\n",
            "1  Jogging  5.012288  11.264028  0.953424\n",
            "2  Jogging  4.903325  10.882658 -0.081722\n",
            "3  Jogging -0.612916  18.496431  3.023717\n",
            "4  Jogging -1.184970  12.108489  7.205164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Balance the dataset by sampling an equal number of records for each activity\n",
        "Walking = df[df['activity'] == 'Walking'].head(3555).copy()\n",
        "Jogging = df[df['activity'] == 'Jogging'].head(3555).copy()\n",
        "Upstairs = df[df['activity'] == 'Upstairs'].head(3555).copy()\n",
        "Downstairs = df[df['activity'] == 'Downstairs'].head(3555).copy()\n",
        "Sitting = df[df['activity'] == 'Sitting'].head(3555).copy()\n",
        "Standing = df[df['activity'] == 'Standing'].copy()\n",
        "\n",
        "balanced_data = pd.DataFrame()\n",
        "balanced_data = pd.concat([balanced_data, Walking, Jogging, Upstairs, Downstairs, Sitting, Standing])\n",
        "\n",
        "# Display the shape and value counts of the balanced dataset\n",
        "print(balanced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRikfQ4kcgbV",
        "outputId": "c895552d-bb9e-4b0a-bd51-2377bdd70815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21330, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = LabelEncoder()\n",
        "balanced_data['label'] = label.fit_transform(balanced_data['activity'])\n",
        "print(balanced_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX-xeq4rclQY",
        "outputId": "c28334ff-a013-4651-f1d0-d122a19ee3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    activity         x          y         z  label\n",
            "597  Walking  0.844462   8.008764  2.792171      5\n",
            "598  Walking  1.116869   8.621680  3.786457      5\n",
            "599  Walking -0.503953  16.657684  1.307553      5\n",
            "600  Walking  4.794363  10.760075 -1.184970      5\n",
            "601  Walking -0.040861   9.234595 -0.694638      5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features and labels\n",
        "X = balanced_data[['x', 'y', 'z']]\n",
        "y = balanced_data['label']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Create a DataFrame for scaled features\n",
        "scaled_X = pd.DataFrame(data=X, columns=['x', 'y', 'z'])\n",
        "scaled_X['label'] = y.values\n",
        "print(scaled_X.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq6qXNQ4cnjM",
        "outputId": "1cff6411-d86b-443f-e978-f440e6d0de01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          x         y         z  label\n",
            "0  0.000503 -0.099190  0.337933      5\n",
            "1  0.073590  0.020386  0.633446      5\n",
            "2 -0.361275  1.588160 -0.103312      5\n",
            "3  1.060258  0.437573 -0.844119      5\n",
            "4 -0.237028  0.139962 -0.698386      5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define frame and hop sizes\n",
        "Fs = 20\n",
        "frame_size = Fs * 4  # 80 samples\n",
        "hop_size = Fs * 2    # 40 samples\n",
        "\n",
        "def get_frames(df, frame_size, hop_size):\n",
        "    N_FEATURES = 3\n",
        "\n",
        "    frames = []\n",
        "    labels = []\n",
        "    for i in range(0, len(df) - frame_size, hop_size):\n",
        "        x = df['x'].values[i: i + frame_size]\n",
        "        y = df['y'].values[i: i + frame_size]\n",
        "        z = df['z'].values[i: i + frame_size]\n",
        "\n",
        "        # Retrieve the most frequent label in the frame\n",
        "        label = stats.mode(df['label'][i: i + frame_size])[0]\n",
        "        # Convert the LabelEncoder object to a Series\n",
        "\n",
        "        frames.append([x, y, z])\n",
        "        labels.append(label)\n",
        "\n",
        "    # Reshape frames to be compatible with CNN input\n",
        "    frames = np.asarray(frames).reshape(-1, frame_size, N_FEATURES)\n",
        "    labels = np.asarray(labels)\n",
        "\n",
        "    return frames, labels\n",
        "\n",
        "# Segment the data into frames\n",
        "X, y = get_frames(scaled_X, frame_size, hop_size)\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CoWYakNcqHb",
        "outputId": "0eea3928-b88e-45d9-dff6-51fafec7ac28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(532, 80, 3) (532,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "# Custom Dataset class for PyTorch\n",
        "class WISDMDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample, label\n",
        "\n",
        "# Convert to PyTorch Datasets\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "dataset = WISDMDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Split into training and testing datasets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "hyt0rytKctCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training dataset size:\", len(train_dataset))\n",
        "print(\"Testing dataset size:\", len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwn3Pazccu3x",
        "outputId": "8e22508f-0985-4489-baf0-5af305b42f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 425\n",
            "Testing dataset size: 107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "zN2_OgIKcxUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Deformable Convolutional Network (DCN): 可变形网络'''\n",
        "def get_min_value(ks, padsize):\n",
        "    ks0 = ks[0]\n",
        "    ks1 = ks[1]\n",
        "    pd0 = padsize[0]\n",
        "    pd1 = padsize[1]\n",
        "    while ks0 - 2 > 1:\n",
        "        if pd0 > 0:\n",
        "            ks0 -= 2\n",
        "            pd0 -= 1\n",
        "        else:\n",
        "            break\n",
        "    while ks1 - 2 > 1:\n",
        "        if pd1 > 0:\n",
        "            ks1 -= 2\n",
        "            pd1 -= 1\n",
        "        else:\n",
        "            break\n",
        "    return (ks0, ks1), (pd0, pd1)\n",
        "\n",
        "class DeformConv2d(nn.Module):\n",
        "    def __init__(self, inc, outc, kernel_size=3, stride=1, padding=1, bias=None, modulation=False, minit=0, pinit=0):\n",
        "\n",
        "        super(DeformConv2d, self).__init__()\n",
        "        self.kernel_size = (kernel_size, kernel_size) if type(kernel_size) == int else kernel_size\n",
        "        self.padding = (padding, padding) if type(padding) == int else padding\n",
        "        self.stride = (stride, stride) if type(stride) == int else stride\n",
        "\n",
        "        self.zero_padding = nn.ZeroPad2d((self.padding[1], self.padding[1], self.padding[0], self.padding[0]))\n",
        "        self.conv = nn.Conv2d(inc, outc, kernel_size=kernel_size, stride=kernel_size, bias=bias)\n",
        "\n",
        "        ks, pd = get_min_value(self.kernel_size, self.padding)\n",
        "        self.p_conv = nn.Conv2d(inc, 2*self.kernel_size[0]*self.kernel_size[1], kernel_size=ks, padding=pd, stride=self.stride)\n",
        "        if pinit:\n",
        "            # nn.init.normal_(self.p_conv.weight, mean=0, std=pinit)\n",
        "            nn.init.constant_(self.p_conv.weight, pinit)\n",
        "        else:\n",
        "            nn.init.constant_(self.p_conv.weight, 0)\n",
        "\n",
        "        self.p_conv.register_backward_hook(self._set_lr)\n",
        "\n",
        "        self.modulation = modulation\n",
        "        if modulation:\n",
        "            self.m_conv = nn.Conv2d(inc, self.kernel_size[0]*self.kernel_size[1], kernel_size=ks, padding=pd, stride=self.stride)\n",
        "            nn.init.constant_(self.m_conv.weight, minit)\n",
        "            self.m_conv.register_backward_hook(self._set_lr)\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_lr(module, grad_input, grad_output):\n",
        "        grad_input = (grad_input[i] * 0.1 for i in range(len(grad_input)))\n",
        "        grad_output = (grad_output[i] * 0.1 for i in range(len(grad_output)))\n",
        "\n",
        "    def forward(self, x):   #(256, 1, 128, 9)\n",
        "        offset = self.p_conv(x)   #(256, 18, 128, 9)\n",
        "        if self.modulation:\n",
        "            m = torch.sigmoid(self.m_conv(x))\n",
        "\n",
        "        dtype = offset.data.type()   #float\n",
        "        ks = self.kernel_size  #(3, 3)\n",
        "        N = offset.size(1) // 2  # 9\n",
        "\n",
        "        if self.padding:\n",
        "            x = self.zero_padding(x)\n",
        "\n",
        "        # (b, 2N, h, w)\n",
        "        p = self._get_p(offset, dtype) #(256, 18, 6, 512)\n",
        "        p = p.contiguous().permute(0, 2, 3, 1) #(256, 6, 512, 18)\n",
        "\n",
        "        q_lt = p.detach().floor() #向下取整\n",
        "        q_rb = q_lt + 1\n",
        "\n",
        "        q_lt = torch.cat([torch.clamp(q_lt[..., :N], 0, x.size(2)-1), torch.clamp(q_lt[..., N:], 0, x.size(3)-1)], dim=-1).long()\n",
        "        q_rb = torch.cat([torch.clamp(q_rb[..., :N], 0, x.size(2)-1), torch.clamp(q_rb[..., N:], 0, x.size(3)-1)], dim=-1).long()\n",
        "        q_lb = torch.cat([q_lt[..., :N], q_rb[..., N:]], dim=-1)\n",
        "        q_rt = torch.cat([q_rb[..., :N], q_lt[..., N:]], dim=-1)\n",
        "\n",
        "        # clip p\n",
        "        p = torch.cat([torch.clamp(p[..., :N], 0, x.size(2)-1), torch.clamp(p[..., N:], 0, x.size(3)-1)], dim=-1)\n",
        "\n",
        "        # bilinear kernel (b, h, w, N)\n",
        "        g_lt = (1 + (q_lt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_lt[..., N:].type_as(p) - p[..., N:]))\n",
        "        g_rb = (1 - (q_rb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_rb[..., N:].type_as(p) - p[..., N:]))\n",
        "        g_lb = (1 + (q_lb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_lb[..., N:].type_as(p) - p[..., N:]))\n",
        "        g_rt = (1 - (q_rt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_rt[..., N:].type_as(p) - p[..., N:]))\n",
        "\n",
        "        # (b, c, h, w, N)\n",
        "        x_q_lt = self._get_x_q(x, q_lt, N)\n",
        "        x_q_rb = self._get_x_q(x, q_rb, N)\n",
        "        x_q_lb = self._get_x_q(x, q_lb, N)\n",
        "        x_q_rt = self._get_x_q(x, q_rt, N)\n",
        "\n",
        "        # (b, c, h, w, N)\n",
        "        x_offset = g_lt.unsqueeze(dim=1) * x_q_lt + \\\n",
        "                   g_rb.unsqueeze(dim=1) * x_q_rb + \\\n",
        "                   g_lb.unsqueeze(dim=1) * x_q_lb + \\\n",
        "                   g_rt.unsqueeze(dim=1) * x_q_rt\n",
        "\n",
        "        # modulation\n",
        "        if self.modulation:\n",
        "            m = m.contiguous().permute(0, 2, 3, 1)\n",
        "            m = m.unsqueeze(dim=1)\n",
        "            m = torch.cat([m for _ in range(x_offset.size(1))], dim=1)\n",
        "            x_offset *= m\n",
        "\n",
        "        x_offset = self._reshape_x_offset(x_offset, ks)\n",
        "        out = self.conv(x_offset)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _get_p_n(self, N, dtype):\n",
        "        p_n_x, p_n_y = torch.meshgrid(\n",
        "            torch.arange(-(self.kernel_size[1]-1)//2, (self.kernel_size[1]-1)//2+1),\n",
        "            torch.arange(-(self.kernel_size[0]-1)//2, (self.kernel_size[0]-1)//2+1))\n",
        "        # (2N, 1)\n",
        "        p_n = torch.cat([torch.flatten(p_n_x), torch.flatten(p_n_y)], 0)\n",
        "        p_n = p_n.view(1, 2*N, 1, 1).type(dtype)\n",
        "\n",
        "        return p_n\n",
        "\n",
        "    def _get_p_0(self, h, w, N, dtype):\n",
        "        p_0_x, p_0_y = torch.meshgrid(\n",
        "            torch.arange(1, h*self.stride[0]+1, self.stride[0]),\n",
        "            torch.arange(1, w*self.stride[1]+1, self.stride[1]))\n",
        "        p_0_x = torch.flatten(p_0_x).view(1, 1, h, w).repeat(1, N, 1, 1)\n",
        "        p_0_y = torch.flatten(p_0_y).view(1, 1, h, w).repeat(1, N, 1, 1)\n",
        "        p_0 = torch.cat([p_0_x, p_0_y], 1).type(dtype)\n",
        "\n",
        "        return p_0\n",
        "\n",
        "    def _get_p(self, offset, dtype):\n",
        "        N, h, w = offset.size(1)//2, offset.size(2), offset.size(3)   #(9, 128, 9)\n",
        "\n",
        "        # (1, 2N, 1, 1)\n",
        "        p_n = self._get_p_n(N, dtype) #(1, 18, 1, 1)\n",
        "        # (1, 2N, h, w)\n",
        "        p_0 = self._get_p_0(h, w, N, dtype) #(1, 9+9, 128, 9)\n",
        "        p = p_0 + p_n + offset\n",
        "        return p\n",
        "\n",
        "    def _get_x_q(self, x, q, N):\n",
        "        b, h, w, _ = q.size()\n",
        "        padded_w = x.size(3)\n",
        "        c = x.size(1)\n",
        "        # (b, c, h*w)\n",
        "        x = x.contiguous().view(b, c, -1)\n",
        "\n",
        "        # (b, h, w, N)\n",
        "        index = q[..., :N]*padded_w + q[..., N:]  # offset_x*w + offset_y\n",
        "        # (b, c, h*w*N)\n",
        "        index = index.contiguous().unsqueeze(dim=1).expand(-1, c, -1, -1, -1).contiguous().view(b, c, -1)\n",
        "\n",
        "        x_offset = x.gather(dim=-1, index=index).contiguous().view(b, c, h, w, N)\n",
        "\n",
        "        return x_offset\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_x_offset(x_offset, ks):\n",
        "        b, c, h, w, N = x_offset.size()\n",
        "        x_offset = torch.cat([x_offset[..., s:s+ks[1]].contiguous().view(b, c, h, w*ks[1]) for s in range(0, N, ks[1])], dim=-1)\n",
        "        x_offset = x_offset.contiguous().view(b, c, h*ks[0], w*ks[1])\n",
        "\n",
        "        return x_offset\n",
        "\n",
        "class DeformableConvolutionalNetwork(nn.Module):\n",
        "    def __init__(self, train_shape, category):\n",
        "        super(DeformableConvolutionalNetwork, self).__init__()\n",
        "        '''\n",
        "            train_shape: 总体训练样本的shape【DCN自适应调整卷积形状，不需要固定按模态轴进行条形卷积，因此这里没有用到train_shape来设定adapool与fc】\n",
        "            category: 类别数\n",
        "        '''\n",
        "        self.layer = nn.Sequential(\n",
        "            DeformConv2d(1, 64, 3, 2, 1, modulation=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            DeformConv2d(64, 128, 3, 2, 1, modulation=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            DeformConv2d(128, 256, 3, 2, 1, modulation=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            DeformConv2d(256, 512, 3, 2, 1, modulation=True),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.ada_pool = nn.AdaptiveAvgPool2d((1, 4))\n",
        "        self.fc = nn.Linear(512*4, category)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "            x.shape: [b, c, series, modal]\n",
        "        '''\n",
        "        x = x.permute(0, 1, 3, 2) # [b, c, modal, series]\n",
        "        x = self.layer(x)\n",
        "        x = self.ada_pool(x) # [b, c, 1, 4]\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kLLXx5XDdHra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeformableConvolutionalNetwork(train_shape=X.shape, category=len(activities))\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQp70R3dSci",
        "outputId": "72f4b951-2d86-453a-a2c1-6523608ab551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeformableConvolutionalNetwork(\n",
            "  (layer): Sequential(\n",
            "    (0): DeformConv2d(\n",
            "      (zero_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "      (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(3, 3), bias=False)\n",
            "      (p_conv): Conv2d(1, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (m_conv): Conv2d(1, 9, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): DeformConv2d(\n",
            "      (zero_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(3, 3), bias=False)\n",
            "      (p_conv): Conv2d(64, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (m_conv): Conv2d(64, 9, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "    (6): DeformConv2d(\n",
            "      (zero_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(3, 3), bias=False)\n",
            "      (p_conv): Conv2d(128, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (m_conv): Conv2d(128, 9, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU()\n",
            "    (9): DeformConv2d(\n",
            "      (zero_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(3, 3), bias=False)\n",
            "      (p_conv): Conv2d(256, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (m_conv): Conv2d(256, 9, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU()\n",
            "  )\n",
            "  (ada_pool): AdaptiveAvgPool2d(output_size=(1, 4))\n",
            "  (fc): Linear(in_features=2048, out_features=6, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    return correct / labels.size(0)"
      ],
      "metadata": {
        "id": "PDbrd5CXdody"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        #print(inputs)\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs.permute(0, 2, 1).unsqueeze(1))  # Permute input dimensions for Conv1d\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the accuracy and loss\n",
        "        running_loss += loss.item()\n",
        "        correct_predictions += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "    # Calculate train accuracy and loss\n",
        "    train_accuracy = correct_predictions / total_predictions\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Print statistics every epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq9vjYUHdsP-",
        "outputId": "79e5003a-6985-44ee-98b6-694b793e91e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [2/50], Train Loss: 0.0002, Train Accuracy: 1.0000\n",
            "Epoch [3/50], Train Loss: 0.0001, Train Accuracy: 1.0000\n",
            "Epoch [4/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [5/50], Train Loss: 0.0001, Train Accuracy: 1.0000\n",
            "Epoch [6/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [7/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [8/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [9/50], Train Loss: 0.0002, Train Accuracy: 1.0000\n",
            "Epoch [10/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [11/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [12/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [13/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [14/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [15/50], Train Loss: 0.0008, Train Accuracy: 1.0000\n",
            "Epoch [16/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [17/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [18/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [19/50], Train Loss: 0.0001, Train Accuracy: 1.0000\n",
            "Epoch [20/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [21/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [22/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [23/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [24/50], Train Loss: 0.0001, Train Accuracy: 1.0000\n",
            "Epoch [25/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [26/50], Train Loss: 0.0001, Train Accuracy: 1.0000\n",
            "Epoch [27/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [28/50], Train Loss: 0.0001, Train Accuracy: 1.0000\n",
            "Epoch [29/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [30/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [31/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [32/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [33/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [34/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [35/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [36/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [37/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [38/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [39/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [40/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [41/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [42/50], Train Loss: 0.0002, Train Accuracy: 1.0000\n",
            "Epoch [43/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [44/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [45/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [46/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [47/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [48/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [49/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
            "Epoch [50/50], Train Loss: 0.0000, Train Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs.permute(0, 2, 1).unsqueeze(1))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = test_correct / test_total\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yulJSsTubH1",
        "outputId": "e5e9e2ba-83d7-423f-c336-760facbfdef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9533\n"
          ]
        }
      ]
    }
  ]
}